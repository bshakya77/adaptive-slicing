{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ab14af-1b39-4973-aa56-dfb1a43bb3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (2.1.2+cu118)\n",
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp39-cp39-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: sahi in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (0.11.22)\n",
      "Requirement already satisfied: yolov8 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: ultralytics in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (8.3.95)\n",
      "Requirement already satisfied: numpy in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: opencv-python in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (4.10.0.84)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from torch) (3.9.0)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from torch) (2023.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Using cached triton-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: click in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from sahi) (8.1.8)\n",
      "Requirement already satisfied: fire in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from sahi) (0.7.0)\n",
      "Requirement already satisfied: pillow>=8.2.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from sahi) (10.2.0)\n",
      "Requirement already satisfied: pybboxes==0.1.6 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from sahi) (0.1.6)\n",
      "Requirement already satisfied: pyyaml in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from sahi) (6.0.1)\n",
      "Requirement already satisfied: requests in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from sahi) (2.31.0)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from sahi) (2.0.7)\n",
      "Requirement already satisfied: terminaltables in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from sahi) (3.1.10)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from sahi) (4.66.1)\n",
      "Requirement already satisfied: yolov5 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from yolov8) (7.0.14)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from ultralytics) (3.8.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from ultralytics) (1.12.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from ultralytics) (0.16.2+cu118)\n",
      "Requirement already satisfied: psutil in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from ultralytics) (2.2.0)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (6.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from requests->sahi) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from requests->sahi) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from requests->sahi) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from requests->sahi) (2023.11.17)\n",
      "Requirement already satisfied: typing-extensions in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: termcolor in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from fire->sahi) (2.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: gitpython>=3.1.30 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from yolov5->yolov8) (3.1.44)\n",
      "Requirement already satisfied: thop>=0.1.1 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from yolov5->yolov8) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from yolov5->yolov8) (2.19.0)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from yolov5->yolov8) (75.8.2)\n",
      "Requirement already satisfied: boto3>=1.19.1 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from yolov5->yolov8) (1.37.4)\n",
      "Requirement already satisfied: huggingface-hub<0.25.0,>=0.12.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from yolov5->yolov8) (0.20.3)\n",
      "Requirement already satisfied: roboflow>=0.2.29 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from yolov5->yolov8) (1.1.54)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.4 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from boto3>=1.19.1->yolov5->yolov8) (1.37.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from boto3>=1.19.1->yolov5->yolov8) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from boto3>=1.19.1->yolov5->yolov8) (0.11.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from gitpython>=3.1.30->yolov5->yolov8) (4.0.12)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from roboflow>=0.2.29->yolov5->yolov8) (4.10.0.84)\n",
      "Requirement already satisfied: python-dotenv in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from roboflow>=0.2.29->yolov5->yolov8) (1.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from roboflow>=0.2.29->yolov5->yolov8) (1.0.0)\n",
      "Requirement already satisfied: filetype in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from roboflow>=0.2.29->yolov5->yolov8) (1.2.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from tensorboard>=2.4.1->yolov5->yolov8) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from tensorboard>=2.4.1->yolov5->yolov8) (1.60.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from tensorboard>=2.4.1->yolov5->yolov8) (3.5.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from tensorboard>=2.4.1->yolov5->yolov8) (4.23.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from tensorboard>=2.4.1->yolov5->yolov8) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from tensorboard>=2.4.1->yolov5->yolov8) (3.0.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /mmfs1/home/dsu.local/bshakya/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5->yolov8) (5.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /mmfs1/cm/shared/apps_local/python/shared_envs/torch11.8/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.4.1->yolov5->yolov8) (7.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch sahi yolov8 ultralytics numpy opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16413d25-727b-4f31-8370-afa858b7c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required functions and classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image, read_image_as_pil\n",
    "from sahi.utils.file import Path, increment_path, list_files, save_json, save_pickle, download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict, agg_prediction, get_prediction_batched, get_sliced_prediction_batched \n",
    "from sahi.prediction import visualize_object_predictions\n",
    "from IPython.display import Image\n",
    "from numpy import asarray\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sahi.prediction import ObjectPrediction, PredictionResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8959033a-43e0-4f7b-baee-8073f7b64f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download YOLOv8-S model to 'models/yolov8s.pt'\n",
    "yolov8_model_path = 'models/yolov8/last.pt'\n",
    "#download_yolov8s_model(destination_path=yolov8_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b740a8f3-f3d5-4657-b0ff-2d39b3c86a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=yolov8_model_path,\n",
    "    confidence_threshold=0.3,\n",
    "    device=\"cuda:0\", # or 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5176c69f-d44e-42e1-a96a-a3d6e03b3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "def get_slice_parameters(object_density, slice_size):\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    #image_path = \"test_data/0000006_06773_d_0000018.jpg\"\n",
    "    #image = Image.open(image_path).convert(\"RGB\")\n",
    "    #image_width, image_height  = image.size\n",
    "    #print(\"Image Width:\", image_width)\n",
    "    #print(\"Image Height:\", image_height)\n",
    "    #min_dim = min(image_width, image_height)\n",
    "    #slice_size = min_dim // 4 if min_dim > 1600 else min_dim // 2\n",
    "    #print(f\"Dimension calculation time taken: {(time.time() - start_time)*1000:.2f} ms\")\n",
    "\n",
    "    \n",
    "    if object_density >= 50:\n",
    "        #slice_size = min_dim // 4\n",
    "        slice_width = slice_size\n",
    "        slice_height = slice_size\n",
    "        overlap_width_ratio = 0.5\n",
    "        overlap_height_ratio = 0.5\n",
    "    elif 25 <= object_density < 50:\n",
    "        #slice_size = min_dim // 2\n",
    "        slice_width = slice_size\n",
    "        slice_height = slice_size\n",
    "        overlap_width_ratio = 0.25\n",
    "        overlap_height_ratio = 0.25\n",
    "    elif 10 <= object_density < 25:\n",
    "        #slice_size = min_dim // 2\n",
    "        slice_width = slice_size\n",
    "        slice_height = slice_size\n",
    "        overlap_width_ratio = 0.15\n",
    "        overlap_height_ratio = 0.15\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    return slice_width, slice_height, overlap_width_ratio, overlap_height_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68995a5b-35ef-41f6-a278-eb71ae19815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get image details by image_id\n",
    "def get_image_id(coco_data, image_name):\n",
    "    for image in coco_data[\"images\"]:\n",
    "        file_name = Path(image['file_name']).stem\n",
    "        if file_name == image_name:\n",
    "            return image['id']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a44df7-d9fa-4877-a217-76fb31290c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sahi.prediction import ObjectPrediction, PredictionResult\n",
    "\n",
    "# export visualization\n",
    "def predict_sliced_images(input_folder, dataset_json_path, detection_model, slice_size):\n",
    "    \"\"\"\n",
    "    Processes all image files in input_folder:\n",
    "      - Runs predictions using get_prediction function and detection_model.\n",
    "      - Saves annotated images with bounding boxes in output_folder.\n",
    "      - Saves prediction details as JSON files in output_folder.\n",
    "    \n",
    "    Parameters:\n",
    "      input_folder (str): Path to the folder containing images.\n",
    "      output_folder (str): Path to the folder where results will be saved.\n",
    "      detection_model: Your detection model used for prediction.\n",
    "    \"\"\"\n",
    "    name = \"exp\"\n",
    "    save_dir = Path(increment_path(Path(\"sliced_predictions\") / name, exist_ok=False))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if dataset_json_path:\n",
    "        with open(dataset_json_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "    \n",
    "    #color = (0, 255, 0)  # original annotations in green\n",
    "    visual_bbox_gt_thickness = 3\n",
    "    visual_bbox_thickness = 2\n",
    "    visual_text_size = 0.5\n",
    "    visual_text_thickness = 1\n",
    "    visual_hide_labels = False\n",
    "    visual_hide_conf = False\n",
    "    visual_export_format = 'png'\n",
    "    sliced_predictions = []\n",
    "    image_ids = []\n",
    "    coco_json = []\n",
    "    \n",
    "    # Loop over files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            image_as_pil = read_image_as_pil(image_path)\n",
    "            filename_without_ext = Path(filename).stem\n",
    "            \n",
    "            print(\"*****************************************\")\n",
    "            print(\"File Name\", filename_without_ext)\n",
    "\n",
    "            img_id = get_image_id(data, filename_without_ext)\n",
    "            #image_ids.append(image_id)\n",
    "            \n",
    "            # Get predictions from your detection model\n",
    "\n",
    "            \n",
    "            time_start = time.time()\n",
    "            prediction = get_prediction(image_path, detection_model) #changes\n",
    "            time_end = time.time() - time_start\n",
    "            #print(f\"Intial Prediction Performed in {time_end} seconds\")\n",
    "            print(\"Intial Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "            \n",
    "            object_density = len(prediction.object_prediction_list)\n",
    "            print(\"Object Density:\", object_density)\n",
    "            \n",
    "            if object_density > 10:\n",
    "                slice_width, slice_height, overlap_width_ratio, overlap_height_ratio = get_slice_parameters(object_density, slice_size)\n",
    "    \n",
    "                print(\"********* Slice Parameters ***********\")\n",
    "                print(\"Slice Width: \", slice_width)\n",
    "                print(\"Slice Height: \", slice_height)\n",
    "                print(\"Overlap Width Ratio: \", overlap_width_ratio)\n",
    "                print(\"Overlap Height Ratio: \", overlap_height_ratio)\n",
    "\n",
    "                time_start_slice = time.time()\n",
    "                result_sahi = get_sliced_prediction(\n",
    "                    image_path,\n",
    "                    detection_model,\n",
    "                    slice_height = slice_height,\n",
    "                    slice_width = slice_width,\n",
    "                    overlap_height_ratio = overlap_height_ratio,\n",
    "                    overlap_width_ratio = overlap_width_ratio,\n",
    "                    postprocess_type = \"NMS\",\n",
    "                    verbose = 2\n",
    "                )\n",
    "                time_end_slice = time.time() - time_start_slice\n",
    "                #print(f\"Prediction Performed in {time_end1} seconds\")\n",
    "                print(\"Sliced Prediction time is: {:.2f} ms\".format(time_end_slice * 1000))\n",
    "                \n",
    "                coco_prediction = result_sahi.to_coco_predictions(image_id=img_id)\n",
    "\n",
    "                for idx, predict in enumerate(coco_prediction):\n",
    "                    if coco_prediction[idx][\"bbox\"]:\n",
    "                            coco_json.append(predict)\n",
    "                    \n",
    "                sliced_predictions.append(result_sahi)\n",
    "                \n",
    "                visualize_object_predictions(\n",
    "                    np.ascontiguousarray(image_as_pil),\n",
    "                    object_prediction_list=result_sahi.object_prediction_list,\n",
    "                    rect_th=visual_bbox_thickness,\n",
    "                    text_size=visual_text_size,\n",
    "                    text_th=visual_text_thickness,\n",
    "                    hide_labels=visual_hide_labels,\n",
    "                    hide_conf=visual_hide_conf,\n",
    "                    output_dir=save_dir,\n",
    "                    file_name=filename_without_ext,\n",
    "                    export_format=visual_export_format,\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                print(\"Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "                \n",
    "                coco_prediction = prediction.to_coco_predictions(image_id=img_id)\n",
    "\n",
    "                for idx, predict in enumerate(coco_prediction):\n",
    "                    if coco_prediction[idx][\"bbox\"]:\n",
    "                            coco_json.append(predict)\n",
    "                    \n",
    "                sliced_predictions.append(prediction)\n",
    "                \n",
    "                visualize_object_predictions(\n",
    "                    np.ascontiguousarray(image_as_pil),\n",
    "                    object_prediction_list=prediction.object_prediction_list,\n",
    "                    rect_th=visual_bbox_thickness,\n",
    "                    text_size=visual_text_size,\n",
    "                    text_th=visual_text_thickness,\n",
    "                    hide_labels=visual_hide_labels,\n",
    "                    hide_conf=visual_hide_conf,\n",
    "                    output_dir=save_dir,\n",
    "                    file_name=filename_without_ext,\n",
    "                    export_format=visual_export_format,\n",
    "                )\n",
    "    total_time = time_end + time_end_slice\n",
    "                 \n",
    "    if dataset_json_path:\n",
    "        save_path = str(save_dir / \"result.json\")\n",
    "        save_json(coco_json, save_path)\n",
    "        print(f\"Prediction results are successfully exported to {save_dir}\")\n",
    "    print(f\"Prediction Completed Sucessfully: {len(sliced_predictions)} images\")\n",
    "    print(\"Total Prediction time is: {:.2f} ms\".format(total_time * 1000))\n",
    "    return sliced_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c3365c6-7c7b-46d0-9ea9-d17ff8db82d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000011_05068_d_0000008\n",
      "Intial Prediction time is: 23.96 ms\n",
      "Object Density: 33\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.25\n",
      "Overlap Height Ratio:  0.25\n",
      "Sliced Boxes Count: 8\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 8 slices.\n",
      "Original Prediction Count 93\n",
      "Final Bounding Box Count (NMS):  31\n",
      "Final Bounding Box Count (NMS):  6\n",
      "Final Bounding Box Count (NMS):  2\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  6\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  6\n",
      "Slicing performed in 0.006385087966918945 seconds.\n",
      "Prediction performed in 0.15878868103027344 seconds.\n",
      "Sliced Prediction time is: 164.20 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp227\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 188.16 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-Optimized-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1162.json\"\n",
    "slice_size = 512\n",
    "result_preds_adapt_opt_nms_iou_size = predict_sliced_images(source_folder, json_path, detection_model, slice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f74ab28a-182d-41a1-84e5-21dedd4a60a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST PROCESSING: NMS\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|██████████| 1/1 [00:00<00:00, 515.40it/s]\n",
      "Performing inference on images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: 0000011_05068_d_0000008\n",
      "Image Size:  (1360, 765)\n",
      "Sliced Boxes Count: 10\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 10 slices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction Count 148\n",
      "Final Bounding Box Count (NMS):  34\n",
      "Final Bounding Box Count (NMS):  9\n",
      "Final Bounding Box Count (NMS):  3\n",
      "Final Bounding Box Count (NMS):  3\n",
      "Final Bounding Box Count (NMS):  7\n",
      "Final Bounding Box Count (NMS):  2\n",
      "Final Bounding Box Count (NMS):  10\n",
      "Prediction time is: 233.24 ms\n",
      "Prediction results are successfully exported to runs/predict/exp228\n",
      "Model loaded in 0.02968883514404297 seconds.\n",
      "Slicing performed in 0.0010364055633544922 seconds.\n",
      "Prediction performed in 0.23323941230773926 seconds.\n",
      "Exporting performed in 0.04594826698303223 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_predict_nms_latest = predict(source='./single_test',\n",
    "                         dataset_json_path = './subset_vis_test_data_1162.json',\n",
    "                         model_type = 'ultralytics',\n",
    "                         model_path = 'models/yolov8/last.pt',\n",
    "                         slice_height = 512,\n",
    "                         slice_width = 512,\n",
    "                         overlap_height_ratio = 0.5,\n",
    "                         overlap_width_ratio = 0.5,\n",
    "                         postprocess_type = \"NMS\",\n",
    "                         verbose = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aee7bf4a-45ca-4ef5-b109-8b0ce1d0be43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000006_06773_d_0000018\n",
      "Intial Prediction time is: 23.62 ms\n",
      "Object Density: 37\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.25\n",
      "Overlap Height Ratio:  0.25\n",
      "POST PROCESS:  TruncatedNMS\n",
      "Performing prediction on 8 slices.\n",
      "Original Prediction Count 110\n",
      "Adaptive Prediction Count 110\n",
      "Total Valid prediction:  34\n",
      "Slicing performed in 0.006056547164916992 seconds.\n",
      "Prediction performed in 0.18665170669555664 seconds.\n",
      "Sliced Prediction time is: 192.41 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp142\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 216.04 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-Optimized-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_visdrone_test_990.json\"\n",
    "slice_size = 512\n",
    "result_preds_adapt_opt_nms_iou_size = predict_sliced_images(source_folder, json_path, detection_model, slice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c10a9a89-a19b-4388-acd0-a5bfac6c84ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.429\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.714\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.424\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.832\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.759\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.426\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.390\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.512\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.249\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.433\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.553\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.286\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp142/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './sliced_predictions/exp142/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71917e85-28a9-4399-ac5a-034f3d4a8ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000006_06773_d_0000018\n",
      "Intial Prediction time is: 27.87 ms\n",
      "Object Density: 37\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.25\n",
      "Overlap Height Ratio:  0.25\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 8 slices.\n",
      "Original Prediction Count 110\n",
      "Filtered Prediction:  41\n",
      "Final Bounding Box Count: 20\n",
      "Filtered Prediction:  8\n",
      "Final Bounding Box Count: 4\n",
      "Filtered Prediction:  54\n",
      "Final Bounding Box Count: 11\n",
      "Filtered Prediction:  5\n",
      "Final Bounding Box Count: 3\n",
      "Filtered Prediction:  2\n",
      "Final Bounding Box Count: 2\n",
      "Slicing performed in 0.0072019100189208984 seconds.\n",
      "Prediction performed in 0.16542506217956543 seconds.\n",
      "Sliced Prediction time is: 171.24 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp140\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 199.11 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-Optimized-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_visdrone_test_990.json\"\n",
    "slice_size = 512\n",
    "result_preds_adapt_opt_nms_iou_size = predict_sliced_images(source_folder, json_path, detection_model, slice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e56761-bd2d-48b3-854b-16fee71d6149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.407\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.674\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.416\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.832\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.675\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.390\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.460\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.350\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.433\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.495\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.400\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp140/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './sliced_predictions/exp140/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c5b5192-de19-4d38-a09e-4e9c69a62af7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000006_06773_d_0000018\n",
      "Intial Prediction time is: 27.14 ms\n",
      "Object Density: 37\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.25\n",
      "Overlap Height Ratio:  0.25\n",
      "POST PROCESS:  TruncatedNMS\n",
      "Performing prediction on 8 slices.\n",
      "Original Prediction Count 110\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False,  True, False, False,  True, False, False, False,  True,  True, False,  True,  True, False])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True, False, False,  True,  True,  True, False,  True,  True,  True,  True,  True,  True, False,  True, False,  True,  True,  True,  True,  True, False,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False,  True, False,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "         True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True, False,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True, False,  True,  True,  True, False,  True,  True, False, False,  True])\n",
      "tensor([ True,  True,  True, False,  True,  True, False,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "tensor([False,  True,  True,  True,  True,  True, False,  True, False,  True,  True,  True, False,  True, False,  True,  True,  True,  True, False,  True,  True,  True,  True, False, False, False])\n",
      "tensor([ True,  True,  True,  True,  True, False, False, False,  True, False,  True,  True,  True, False,  True,  True,  True])\n",
      "tensor([ True,  True,  True, False,  True,  True,  True,  True,  True, False,  True])\n",
      "tensor([ True,  True,  True,  True,  True,  True, False,  True])\n",
      "tensor([True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True])\n",
      "tensor([False,  True, False,  True])\n",
      "tensor([True])\n",
      "Truncated Final Bounding Box Count: 24\n",
      "Slicing performed in 0.006816864013671875 seconds.\n",
      "Prediction performed in 0.16932916641235352 seconds.\n",
      "Sliced Prediction time is: 174.51 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp139\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 201.64 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-Optimized-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_visdrone_test_990.json\"\n",
    "slice_size = 512\n",
    "result_preds_adapt_opt_nms_iou_size = predict_sliced_images(source_folder, json_path, detection_model, slice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11791b2-b0f5-46f5-b573-a397a58e5d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.356\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.613\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.379\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.832\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.639\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.426\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.336\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.429\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.249\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.367\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.453\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.286\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp139/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './sliced_predictions/exp139/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16123ab2-b1dd-4a2c-b690-911e4a0527e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.211\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.360\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.213\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.242\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.697\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.417\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.106\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.371\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.295\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.127\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.385\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.333\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp134/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp134/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575b9b76-244b-43fd-88cf-29bc282711e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.211\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.360\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.213\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.242\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.697\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.417\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.106\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.371\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.295\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.127\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.385\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.333\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp134/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp134/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd4a7f7-75b3-4cc0-b412-296faa9cd91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000006_06773_d_0000018\n",
      "Intial Prediction time is: 26.86 ms\n",
      "Object Density: 37\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.25\n",
      "Overlap Height Ratio:  0.25\n",
      "POST PROCESS:  TruncatedNMS\n",
      "Performing prediction on 8 slices.\n",
      "Original Prediction Count 110\n",
      "Truncated Final Bounding Box Count: 28\n",
      "Slicing performed in 0.007302045822143555 seconds.\n",
      "Prediction performed in 0.1595766544342041 seconds.\n",
      "Sliced Prediction time is: 165.10 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp135\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 191.96 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-Optimized-NMS\n",
    "source_folder = './single_test/images_bk'\n",
    "json_path = \"./subset_visdrone_test_990.json\"\n",
    "slice_size = 512\n",
    "result_preds_adapt_opt_nms_iou_size = predict_sliced_images(source_folder, json_path, detection_model, slice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e29bae4a-7fd5-43ea-8a53-dbe173f87135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.396\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.685\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.397\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.675\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.426\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.407\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.460\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.249\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.495\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.286\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp135/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './sliced_predictions/exp135/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f06c7e6c-f486-4e33-bc87-9c7165bb2c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST PROCESSING: NMS\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|██████████| 1/1 [00:00<00:00, 409.12it/s]\n",
      "Performing inference on images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: 0000006_06773_d_0000018\n",
      "Image Size:  (1360, 765)\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 10 slices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Valid prediction:  1\n",
      "Total Valid prediction:  21\n",
      "Total Valid prediction:  8\n",
      "Total Valid prediction:  13\n",
      "Total Valid prediction:  3\n",
      "Total Valid prediction:  2\n",
      "Prediction time is: 402.36 ms\n",
      "Prediction results are successfully exported to runs/predict/exp172\n",
      "Model loaded in 0.031191587448120117 seconds.\n",
      "Slicing performed in 0.0011489391326904297 seconds.\n",
      "Prediction performed in 0.4023559093475342 seconds.\n",
      "Exporting performed in 0.05055832862854004 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_predict_nms_1 = predict(source='./single_test',\n",
    "                         dataset_json_path = './subset_visdrone_test_990.json',\n",
    "                         model_type = 'ultralytics',\n",
    "                         model_path = 'models/yolov8/last.pt',\n",
    "                         slice_height = 512,\n",
    "                         slice_width = 512,\n",
    "                         overlap_height_ratio = 0.5,\n",
    "                         overlap_width_ratio = 0.5,\n",
    "                         postprocess_type = \"NMS\",\n",
    "                         verbose = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7b35aed-90d2-45e7-bcd3-42663af0489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.399\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.699\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.376\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.672\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.407\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.446\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.356\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.490\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.400\n",
      "COCO evaluation results are successfully exported to runs/predict/exp172/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './runs/predict/exp172/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "065fa8f0-f835-4344-b87b-8bfc94195238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST PROCESSING: NMS\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|██████████| 1/1 [00:00<00:00, 362.11it/s]\n",
      "Performing inference on images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: 0000006_06773_d_0000018\n",
      "Image Size:  (1360, 765)\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 10 slices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images: 100%|██████████| 1/1 [00:00<00:00,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Valid prediction:  1\n",
      "Total Valid prediction:  21\n",
      "Total Valid prediction:  8\n",
      "Total Valid prediction:  13\n",
      "Total Valid prediction:  3\n",
      "Total Valid prediction:  2\n",
      "Prediction time is: 239.31 ms\n",
      "Prediction results are successfully exported to runs/predict/exp173\n",
      "Model loaded in 0.07065629959106445 seconds.\n",
      "Slicing performed in 0.0011546611785888672 seconds.\n",
      "Prediction performed in 0.23931050300598145 seconds.\n",
      "Exporting performed in 0.04933500289916992 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_predict_nms_2 = predict(source='./single_test',\n",
    "                         dataset_json_path = './subset_visdrone_test_990.json',\n",
    "                         model_type = 'ultralytics',\n",
    "                         model_path = 'models/yolov8/last.pt',\n",
    "                         slice_height = 512,\n",
    "                         slice_width = 512,\n",
    "                         overlap_height_ratio = 0.5,\n",
    "                         overlap_width_ratio = 0.5,\n",
    "                         postprocess_type = \"NMS\",\n",
    "                         verbose = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48f875d9-2425-4e1c-a7b6-4ccb430ceab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST PROCESSING: NMS\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|██████████| 1/1 [00:00<00:00, 377.19it/s]\n",
      "Performing inference on images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: 0000006_06773_d_0000018\n",
      "Image Size:  (1360, 765)\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 10 slices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Valid prediction:  1\n",
      "Total Valid prediction:  21\n",
      "Total Valid prediction:  8\n",
      "Total Valid prediction:  13\n",
      "Total Valid prediction:  3\n",
      "Total Valid prediction:  2\n",
      "Prediction time is: 233.85 ms\n",
      "Prediction results are successfully exported to runs/predict/exp174\n",
      "Model loaded in 0.0613703727722168 seconds.\n",
      "Slicing performed in 0.0011582374572753906 seconds.\n",
      "Prediction performed in 0.23385357856750488 seconds.\n",
      "Exporting performed in 0.04917120933532715 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_predict_nms_3 = predict(source='./single_test',\n",
    "                         dataset_json_path = './subset_visdrone_test_990.json',\n",
    "                         model_type = 'ultralytics',\n",
    "                         model_path = 'models/yolov8/last.pt',\n",
    "                         slice_height = 512,\n",
    "                         slice_width = 512,\n",
    "                         overlap_height_ratio = 0.5,\n",
    "                         overlap_width_ratio = 0.5,\n",
    "                         postprocess_type = \"NMS\",\n",
    "                         verbose = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55938fc3-8af5-4d93-9086-e22e3a0dd68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.399\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.699\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.376\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.672\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.407\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.446\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.356\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.490\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.400\n",
      "COCO evaluation results are successfully exported to runs/predict/exp174/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './runs/predict/exp174/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a94e077f-6fa0-4eda-b144-d7dc4d15cfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.399\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.699\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.376\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.672\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.407\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.446\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.356\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.490\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.400\n",
      "COCO evaluation results are successfully exported to runs/predict/exp173/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './runs/predict/exp173/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22fd03fb-a326-4107-a3e7-dc6ab2e71016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected boxes: [[10, 10, 50, 50]]\n"
     ]
    }
   ],
   "source": [
    "def area(box):\n",
    "    \"\"\"\n",
    "    Compute the area of a box.\n",
    "    Box format: [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    width = max(0, box[2] - box[0])\n",
    "    height = max(0, box[3] - box[1])\n",
    "    return width * height\n",
    "\n",
    "def intersection_area(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Compute the area of intersection between two boxes.\n",
    "    \"\"\"\n",
    "    x_left   = max(boxA[0], boxB[0])\n",
    "    y_top    = max(boxA[1], boxB[1])\n",
    "    x_right  = min(boxA[2], boxB[2])\n",
    "    y_bottom = min(boxA[3], boxB[3])\n",
    "    \n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0\n",
    "    return (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Compute the Intersection-over-Union (IOU) between two boxes.\n",
    "    \"\"\"\n",
    "    inter = intersection_area(boxA, boxB)\n",
    "    union = area(boxA) + area(boxB) - inter\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return inter / union\n",
    "\n",
    "def truncated_nms(boxes, scores, IOUt, IOIt, IOOt):\n",
    "    \"\"\"\n",
    "    Performs Truncated Non-Maximum Suppression.\n",
    "    \n",
    "    Inputs:\n",
    "      boxes: list of detection boxes (each box is [x1, y1, x2, y2])\n",
    "      scores: list of detection scores corresponding to each box\n",
    "      IOUt: IOU threshold for union\n",
    "      IOIt: inside box threshold (for ratio of intersection area to box M)\n",
    "      IOOt: outside box threshold (for ratio of intersection area to box b_i)\n",
    "    \n",
    "    Returns:\n",
    "      K: list of boxes selected after suppression.\n",
    "    \n",
    "    Note:\n",
    "      This implementation follows one interpretation of the provided pseudocode.\n",
    "    \"\"\"\n",
    "    K = []\n",
    "    # Work with a list of (box, score) pairs for easier management.\n",
    "    boxes_scores = list(zip(boxes, scores))\n",
    "    \n",
    "    while boxes_scores:\n",
    "        # Select box M with the highest score.\n",
    "        idx = max(range(len(boxes_scores)), key=lambda i: boxes_scores[i][1])\n",
    "        M, score_M = boxes_scores.pop(idx)\n",
    "        \n",
    "        new_boxes_scores = []\n",
    "        added_M = False  # flag to ensure M is added only once.\n",
    "        \n",
    "        # Process each remaining box b_i.\n",
    "        for box, score in boxes_scores:\n",
    "            inter = intersection_area(M, box)\n",
    "            area_M = area(M)\n",
    "            area_box = area(box)\n",
    "            \n",
    "            # To avoid division by zero, if area is zero, set conditions to False.\n",
    "            if area_M == 0 or area_box == 0:\n",
    "                cond1 = False\n",
    "                cond2 = False\n",
    "            else:\n",
    "                # Compute the ratios.\n",
    "                ratio_M = inter / area_M      # Intersection relative to M.\n",
    "                ratio_box = inter / area_box  # Intersection relative to b_i.\n",
    "                \n",
    "                cond1 = (ratio_M > IOIt) and (ratio_box <= IOOt)\n",
    "                cond2 = (ratio_M <= IOOt) and (ratio_box > IOIt)\n",
    "            \n",
    "            cond = cond1 or cond2\n",
    "            current_iou = iou(M, box)\n",
    "            \n",
    "            if not cond1:\n",
    "                # In this branch the pseudocode suggests to add M to K.\n",
    "                if not added_M:\n",
    "                    K.append(M)\n",
    "                    added_M = True\n",
    "                # If IOU is low and the combined condition is false, keep b_i.\n",
    "                if current_iou <= IOUt and (not cond):\n",
    "                    new_boxes_scores.append((box, score))\n",
    "                # Otherwise, b_i is removed (i.e. not appended).\n",
    "            else:\n",
    "                # If cond1 is true, then remove b_i only if IOU is high.\n",
    "                if current_iou < IOUt:\n",
    "                    new_boxes_scores.append((box, score))\n",
    "                # If IOU >= IOUt, b_i is dropped.\n",
    "        \n",
    "        boxes_scores = new_boxes_scores  # Update the list with surviving boxes.\n",
    "    \n",
    "    return K\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example boxes and scores (format: [x1, y1, x2, y2])\n",
    "    boxes = [\n",
    "        [10, 10, 50, 50],\n",
    "        [12, 12, 48, 48],\n",
    "        [60, 60, 100, 100]\n",
    "    ]\n",
    "    scores = [0.9, 0.85, 0.8]\n",
    "    \n",
    "    # Thresholds (example values)\n",
    "    IOUt = 0.5   # IOU threshold for union\n",
    "    IOIt = 0.6   # inside box threshold\n",
    "    IOOt = 0.4   # outside box threshold\n",
    "    \n",
    "    selected_boxes = truncated_nms(boxes, scores, IOUt, IOIt, IOOt)\n",
    "    print(\"Selected boxes:\", selected_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46aec7cf-41b7-4945-ad80-b6565d4f3997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Prediction Count: 3\n",
      "Final Bounding Box Count: 1\n",
      "Kept prediction indices: [0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def area(box):\n",
    "    \"\"\"\n",
    "    Compute the area of a box.\n",
    "    Box format: [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    width = max(0, box[2] - box[0])\n",
    "    height = max(0, box[3] - box[1])\n",
    "    return width * height\n",
    "\n",
    "def intersection_area(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Compute the intersection area between two boxes.\n",
    "    \"\"\"\n",
    "    x_left   = max(boxA[0], boxB[0])\n",
    "    y_top    = max(boxA[1], boxB[1])\n",
    "    x_right  = min(boxA[2], boxB[2])\n",
    "    y_bottom = min(boxA[3], boxB[3])\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0\n",
    "    return (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Compute the Intersection-over-Union (IOU) between two boxes.\n",
    "    \"\"\"\n",
    "    inter = intersection_area(boxA, boxB)\n",
    "    union = area(boxA) + area(boxB) - inter\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return inter / union\n",
    "\n",
    "def optimized_truncated_nms(predictions: torch.Tensor, \n",
    "                            adaptive_iou: torch.Tensor, \n",
    "                            match_metric: str = \"IOU\", \n",
    "                            conf_threshold: float = 0.3, \n",
    "                            min_area: float = 64,\n",
    "                            IOIt: float = 0.6, \n",
    "                            IOOt: float = 0.4):\n",
    "    \"\"\"\n",
    "    Integrated Optimized and Truncated NMS.\n",
    "    \n",
    "    This function first assumes that predictions have been filtered \n",
    "    (e.g., by confidence and area) and then applies a truncated suppression \n",
    "    logic using both an adaptive IoU threshold (per candidate) and additional \n",
    "    criteria (IOIt, IOOt) for inside/outside box ratios.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Tensor of shape [num_boxes, 6] (x1, y1, x2, y2, score, category_id).\n",
    "        adaptive_iou: Tensor of adaptive IoU thresholds for each prediction.\n",
    "        match_metric: \"IOU\" or \"IOS\". (Currently only \"IOU\" is used in the truncated logic.)\n",
    "        conf_threshold: Confidence threshold (assumes predictions are already filtered).\n",
    "        min_area: Minimum area required for a valid prediction.\n",
    "        IOIt: Threshold for the intersection ratio relative to the selected box (M).\n",
    "        IOOt: Threshold for the intersection ratio relative to a candidate box.\n",
    "        \n",
    "    Returns:\n",
    "        A list of indices (referring to the input predictions) for the kept predictions.\n",
    "    \"\"\"\n",
    "    print(\"Filtered Prediction Count:\", len(predictions))\n",
    "    if predictions.numel() == 0:\n",
    "        return []\n",
    "\n",
    "    # (Optional) You could filter predictions here based on conf_threshold and min_area.\n",
    "    # For this integrated implementation, we assume predictions are already filtered.\n",
    "\n",
    "    # Convert boxes and scores from predictions to Python lists.\n",
    "    # We assume predictions[:, :4] holds [x1, y1, x2, y2] and predictions[:, 4] holds scores.\n",
    "    boxes = predictions[:, :4].tolist()\n",
    "    scores = predictions[:, 4].tolist()\n",
    "    # Track original indices so we can return them.\n",
    "    indices = list(range(len(boxes)))\n",
    "    # Build a list of tuples: (box, score, index)\n",
    "    boxes_scores = list(zip(boxes, scores, indices))\n",
    "\n",
    "    kept_indices = []\n",
    "\n",
    "    while boxes_scores:\n",
    "        # Select the box M with the highest score.\n",
    "        idx = max(range(len(boxes_scores)), key=lambda i: boxes_scores[i][1])\n",
    "        M, score_M, index_M = boxes_scores.pop(idx)\n",
    "        new_boxes_scores = []\n",
    "        added_M = False  # Ensure M is added only once to the kept list.\n",
    "\n",
    "        for (box, score, index) in boxes_scores:\n",
    "            inter = intersection_area(M, box)\n",
    "            a_M = area(M)\n",
    "            a_box = area(box)\n",
    "            if a_M == 0 or a_box == 0:\n",
    "                cond1 = False\n",
    "                cond2 = False\n",
    "            else:\n",
    "                ratio_M = inter / a_M      # Intersection ratio relative to M.\n",
    "                ratio_box = inter / a_box  # Intersection ratio relative to the candidate.\n",
    "                cond1 = (ratio_M > IOIt) and (ratio_box <= IOOt)\n",
    "                cond2 = (ratio_M <= IOOt) and (ratio_box > IOIt)\n",
    "            cond = cond1 or cond2\n",
    "            current_iou = iou(M, box)\n",
    "            # Use the candidate's adaptive IoU threshold.\n",
    "            # (If adaptive_iou is not available for an index, a default (e.g. 0.5) could be used.)\n",
    "            #adaptive_thresh = adaptive_iou[index].item() if adaptive_iou.numel() > index else 0.5\n",
    "\n",
    "            if not cond1:\n",
    "                if not added_M:\n",
    "                    kept_indices.append(index_M)\n",
    "                    added_M = True\n",
    "                # If the overlap is within the adaptive threshold and the extra condition is false, keep candidate.\n",
    "                if current_iou <= adaptive_thresh and (not cond):\n",
    "                    new_boxes_scores.append((box, score, index))\n",
    "            else:\n",
    "                # If cond1 is true, keep the candidate only if IOU is below the adaptive threshold.\n",
    "                if current_iou < adaptive_thresh:\n",
    "                    new_boxes_scores.append((box, score, index))\n",
    "        boxes_scores = new_boxes_scores\n",
    "\n",
    "    print(\"Final Bounding Box Count:\", len(kept_indices))\n",
    "    return kept_indices\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an example predictions tensor.\n",
    "    # Each row: [x1, y1, x2, y2, score, category_id]\n",
    "    predictions = torch.tensor([\n",
    "        [10, 10, 50, 50, 0.9, 1],\n",
    "        [12, 12, 48, 48, 0.85, 1],\n",
    "        [60, 60, 100, 100, 0.8, 2]\n",
    "    ])\n",
    "    # For demonstration, set an adaptive IoU threshold per prediction.\n",
    "    adaptive_iou = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)\n",
    "    \n",
    "    kept = optimized_truncated_nms(predictions, adaptive_iou,\n",
    "                                   match_metric=\"IOU\", \n",
    "                                   conf_threshold=0.3, \n",
    "                                   min_area=64, \n",
    "                                   IOIt=0.6, \n",
    "                                   IOOt=0.4)\n",
    "    print(\"Kept prediction indices:\", kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e5ed40-ade9-4358-a653-f1e68b838f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def truncated_nms(predictions: torch.tensor,\n",
    "                  IoU_threshold:float,\n",
    "                  IoI_threshold:float,\n",
    "                  IoO_threshold:float,\n",
    "                  match_metric: str = \"IOU\", \n",
    "                  conf_threshold: float = 0.3, \n",
    "                  min_area: float = 64):\n",
    "    \"\"\"\n",
    "    Optimized NMS that first filters out low-confidence and small-area predictions.\n",
    "    Args:\n",
    "        predictions: Tensor of shape [num_boxes, 6] (x1, y1, x2, y2, score, category_id).\n",
    "        match_metric: \"IOU\" or \"IOS\".\n",
    "        match_threshold: IoU/IOS threshold for suppression.\n",
    "        conf_threshold: Confidence threshold to filter predictions.\n",
    "        min_area: Minimum area required for a valid prediction.\n",
    "    Returns:\n",
    "        A list of indices for the kept predictions.\n",
    "    \"\"\"\n",
    "    # Filter out low-quality predictions first\n",
    "    #print(\"Prediction Count Original: \", len(predictions))\n",
    "    #predictions = filter_predictions(predictions, conf_threshold, 64)\n",
    "   \n",
    "    #predictions = filter_border_predictions(predictions, image_size, 5)\n",
    "    \n",
    "    if predictions.numel() == 0:\n",
    "        return []\n",
    "    \n",
    "    x1 = predictions[:, 0]\n",
    "    y1 = predictions[:, 1]\n",
    "    x2 = predictions[:, 2]\n",
    "    y2 = predictions[:, 3]\n",
    "    scores = predictions[:, 4]\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    \n",
    "    # Sort the predictions by their confidence scores (ascending order)\n",
    "    order = scores.argsort()\n",
    "    keep = []\n",
    "\n",
    "    while order.numel() > 0:\n",
    "        indices_to_remove = []\n",
    "        idx = order[-1]  # index of highest score prediction\n",
    "        bm = predictions[idx, :4] #prediction with highest score\n",
    "        bm_area = areas[idx]\n",
    "        current_idx = idx.item()\n",
    "        added_bm = False \n",
    "\n",
    "        # Remove the chosen box from the order\n",
    "        order = order[:-1]\n",
    "        if order.numel() == 0:\n",
    "            keep.append(current_idx)\n",
    "            break\n",
    "\n",
    "        # Gather remaining boxes\n",
    "        xx1 = torch.index_select(x1, 0, order)\n",
    "        yy1 = torch.index_select(y1, 0, order)\n",
    "        xx2 = torch.index_select(x2, 0, order)\n",
    "        yy2 = torch.index_select(y2, 0, order)\n",
    "\n",
    "        # Compute intersection coordinates\n",
    "        xx1 = torch.max(xx1, x1[idx])\n",
    "        yy1 = torch.max(yy1, y1[idx])\n",
    "        xx2 = torch.min(xx2, x2[idx])\n",
    "        yy2 = torch.min(yy2, y2[idx])\n",
    "\n",
    "        # Compute width and height of intersection\n",
    "        w = torch.clamp(xx2 - xx1, min=0.0)\n",
    "        h = torch.clamp(yy2 - yy1, min=0.0)\n",
    "        inter = w * h\n",
    "\n",
    "        rem_areas = torch.index_select(areas, 0, order)\n",
    "\n",
    "        # Calculate the two ratios for truncated NMS logic:\n",
    "        ratio1 = inter / (bm_area + 1e-9) # Intersection relative to bm's area\n",
    "        ratio2 = inter / (rem_areas + 1e-9) # Intersection relative to each remaining box's area\n",
    "  \n",
    "        #iou_thresholds = torch.index_select(adaptive_iou, 0, order)\n",
    "        \n",
    "        #print(\"adaptive_iou\", iou_thresholds)\n",
    "        # Keep boxes with IoU/IOS less than their respective adaptive threshold\n",
    "        #mask = match_metric_value < iou_thresholds\n",
    "        \n",
    "        #Truncated NMS Conditions\n",
    "        #This checks if a significant portion of 𝑀 overlaps 𝑏𝑖\n",
    "        cond1 = (ratio1 > IoI_threshold) & (ratio2 <= IoO_threshold)\n",
    "\n",
    "        #This is complementary to Condition 1. Alternatively, if the fraction of bm covered is less than or equal to IoO_threshold while the remaining box's overlap is above IoI_threshold.\n",
    "\n",
    "        cond2 = (ratio1 <= IoO_threshold) & (ratio2 > IoI_threshold)\n",
    "\n",
    "        cond = (cond1 | cond2)\n",
    "\n",
    "        if match_metric == \"IOU\":\n",
    "            union = (rem_areas - inter) + areas[idx]\n",
    "            match_metric_value = inter / (union + 1e-9)\n",
    "        elif match_metric == \"IOS\":\n",
    "            smaller = torch.min(rem_areas, areas[idx])\n",
    "            match_metric_value = inter / (smaller + 1e-9)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported match_metric. Use 'IOU' or 'IOS'.\")\n",
    "\n",
    "        #We add only outside boxes and remove inside boxes\n",
    "        \n",
    "        #if torch.any(~(cond)):  #bm is an outside box          #if not torch.all(cond):\n",
    "        #    keep.append(current_idx)\n",
    "        \n",
    "        if not cond1.any():\n",
    "            if not added_bm:\n",
    "                keep.append(current_idx)\n",
    "                added_bm = True\n",
    "\n",
    "        # Keep boxes with IoU/IOS less than the threshold\n",
    "        mask1 = match_metric_value <= IoU_threshold\n",
    "        mask2 = match_metric_value >= IoU_threshold\n",
    "            # If the overlap is within the adaptive threshold and the extra condition is false, keep candidate.\n",
    "            #if mask1 & (not torch.all(cond)):\n",
    "             #   print(\"step2\")\n",
    "              #  order = order[mask1]\n",
    "        #else:\n",
    "          #  if mask2:\n",
    "           #      print(\"step3\") \n",
    "            #     order = order[mask2]\n",
    "        \n",
    "        keep_mask = (match_metric_value <= IoU_threshold) & (~cond)\n",
    "        #print(keep_mask)\n",
    "        order = order[keep_mask]\n",
    "        \n",
    "    print(\"Truncated Final Bounding Box Count:\", len(keep))\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "619e4ee3-9489-41d6-8433-37eb59f56495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated Final Bounding Box Count: 2\n",
      "Kept prediction indices: [0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an example predictions tensor.\n",
    "    # Each row: [x1, y1, x2, y2, score, category_id]\n",
    "    predictions = torch.tensor([\n",
    "        [10, 10, 50, 50, 0.9, 1],\n",
    "        [12, 12, 48, 48, 0.85, 1],\n",
    "        [60, 60, 100, 100, 0.8, 2]\n",
    "    ])\n",
    "    # For demonstration, set an adaptive IoU threshold per prediction.\n",
    "    adaptive_iou = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)\n",
    "    \n",
    "    kept = truncated_nms(predictions, IoU_threshold = 0.5,\n",
    "                                   match_metric=\"IOU\", \n",
    "                                   conf_threshold=0.3, \n",
    "                                   min_area=64, \n",
    "                                   IoI_threshold=0.6, \n",
    "                                   IoO_threshold=0.4)\n",
    "    print(\"Kept prediction indices:\", kept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ae89df-37d7-4ec2-ab45-7f505ff5547d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Boxes: [(0, 0, 50, 50)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection Over Union (IOU) between two bounding boxes.\n",
    "    \n",
    "    box1 and box2 are tuples (x1, y1, x2, y2) representing the coordinates of the top-left and bottom-right corners.\n",
    "    \"\"\"\n",
    "    x1_intersection = max(box1[0], box2[0])\n",
    "    y1_intersection = max(box1[1], box2[1])\n",
    "    x2_intersection = min(box1[2], box2[2])\n",
    "    y2_intersection = min(box1[3], box2[3])\n",
    "\n",
    "    # Calculate area of intersection\n",
    "    intersection_width = max(0, x2_intersection - x1_intersection)\n",
    "    intersection_height = max(0, y2_intersection - y1_intersection)\n",
    "    intersection_area = intersection_width * intersection_height\n",
    "\n",
    "    # Calculate area of both boxes\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # IOU = intersection area / union area\n",
    "    union_area = area1 + area2 - intersection_area\n",
    "    return intersection_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def truncated_nms(B, S, IOUt, IOIt, IOOt):\n",
    "    \"\"\"\n",
    "    Perform truncated Non-Maximum Suppression (NMS) on the list of bounding boxes and their scores.\n",
    "    \n",
    "    Parameters:\n",
    "        B: List of bounding boxes.\n",
    "        S: List of corresponding detection scores.\n",
    "        IOUt: IOU threshold to keep boxes.\n",
    "        IOIt: Inside box threshold for truncated NMS.\n",
    "        IOOt: Outside box threshold for truncated NMS.\n",
    "        \n",
    "    Returns:\n",
    "        K: List of bounding boxes after truncated NMS.\n",
    "    \"\"\"\n",
    "    K = []  # Initialize the list of selected boxes\n",
    "\n",
    "    while B:\n",
    "        # Select the box with the highest score\n",
    "        m = np.argmax(S)\n",
    "        M = B[m]\n",
    "        \n",
    "        # Remove the selected box from the list\n",
    "        B.pop(m)\n",
    "        S.pop(m)\n",
    "        \n",
    "        # Iterate over the remaining boxes\n",
    "        for i, bi in enumerate(B[:]):\n",
    "            # Check if the intersection condition is met\n",
    "            intersection_area = iou(M, bi)\n",
    "            condition1 = intersection_area > IOIt and intersection_area < iou(M, bi) <= IOOt\n",
    "            condition2 = intersection_area <= IOOt and intersection_area > IOIt\n",
    "            \n",
    "            condition = condition1 or condition2\n",
    "\n",
    "            if not condition:\n",
    "                K.append(M)  # Keep the box in K\n",
    "                if iou(M, bi) <= IOUt and not condition:\n",
    "                    # Remove box if IOU condition fails\n",
    "                    B.pop(i)\n",
    "                    S.pop(i)\n",
    "                else:\n",
    "                    # Otherwise, remove the box and score from B and S\n",
    "                    B.pop(i)\n",
    "                    S.pop(i)\n",
    "            else:\n",
    "                # Keep the box for further iteration\n",
    "                if iou(M, bi) >= IOUt:\n",
    "                    B.pop(i)\n",
    "                    S.pop(i)\n",
    "                \n",
    "    return K\n",
    "\n",
    "# Example usage\n",
    "B = [(0, 0, 50, 50), (10, 10, 60, 60), (30, 30, 80, 80)]  # Example bounding boxes\n",
    "S = [0.9, 0.85, 0.8]  # Corresponding detection scores\n",
    "IOUt = 0.5  # IOU threshold\n",
    "IOIt = 0.3  # Inside box threshold\n",
    "IOOt = 0.7  # Outside box threshold\n",
    "\n",
    "K = truncated_nms(B, S, IOUt, IOIt, IOOt)\n",
    "print(\"Selected Boxes:\", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13ae55e2-5b53-44bf-9287-450ca72e2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def intersection(box1, box2):\n",
    "    \"\"\"Computes the intersection area between two bounding boxes.\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter_width = max(0, x2 - x1)\n",
    "    inter_height = max(0, y2 - y1)\n",
    "    \n",
    "    return inter_width * inter_height\n",
    "\n",
    "def iou(box1, box2):\n",
    "    \"\"\"Computes the Intersection over Union (IoU) between two bounding boxes.\"\"\"\n",
    "    inter = intersection(box1, box2)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area1 + area2 - inter\n",
    "    return inter / union if union > 0 else 0\n",
    "\n",
    "def truncated_nms(predictions:torch.tensor, IOUt, IOIt, IOOt):\n",
    "    \"\"\"\n",
    "    Performs Truncated Non-Maximum Suppression.\n",
    "    \n",
    "    Args:\n",
    "        B (list of list): List of bounding boxes [x1, y1, x2, y2].\n",
    "        S (list of float): Corresponding detection scores.\n",
    "        IOUt (float): IoU threshold.\n",
    "        IOIt (float): Intersection over Inside Box threshold.\n",
    "        IOOt (float): Intersection over Outside Box threshold.\n",
    "    \n",
    "    Returns:\n",
    "        list: Filtered list of bounding boxes after Truncated NMS.\n",
    "    \"\"\"\n",
    "    K = []  # Output list of selected bounding boxes\n",
    "    num_boxes = predictions.shape[0]\n",
    "    print(num_boxes)\n",
    "    \n",
    "    for idx in range(len(predictions)):\n",
    "        B = predictions[idx, :4]\n",
    "        S = predictions[idx, 4]\n",
    "\n",
    "        while len(B) > 0:\n",
    "            m = np.argmax(S)  # Get index of highest score\n",
    "            M = B[m]  # Select the corresponding box\n",
    "            K.append(M)  # Add it to the final selection\n",
    "            \n",
    "            B.pop(m)  # Remove selected box from B\n",
    "            S.pop(m)  # Remove corresponding score\n",
    "    \n",
    "            new_B = []\n",
    "            new_S = []\n",
    "            \n",
    "            for i in range(len(B)):\n",
    "                bi = B[i]\n",
    "                inter = intersection(M, bi)\n",
    "                \n",
    "                # Compute conditions\n",
    "                condition1 = (inter / ((M[2] - M[0]) * (M[3] - M[1])) > IOIt) and (inter / ((bi[2] - bi[0]) * (bi[3] - bi[1])) <= IOOt)\n",
    "                condition2 = (inter / ((M[2] - M[0]) * (M[3] - M[1])) <= IOOt) and (inter / ((bi[2] - bi[0]) * (bi[3] - bi[1])) > IOIt)\n",
    "                condition = condition1 or condition2\n",
    "    \n",
    "                if iou(M, bi) <= IOUt and not condition:\n",
    "                    new_B.append(bi)\n",
    "                    new_S.append(S[i])\n",
    "                elif iou(M, bi) >= IOUt:\n",
    "                    continue  # Discard the box\n",
    "    \n",
    "            B = new_B\n",
    "            S = new_S\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241eaa4-0d30-4472-bb83-2556264ea948",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.tensor([\n",
    "    [10, 10, 50, 50, 0.9, 1],\n",
    "    [12, 12, 48, 48, 0.85, 1],\n",
    "    [60, 60, 100, 100, 0.8, 2]\n",
    "])\n",
    "IOUt = 0.5\n",
    "IOIt = 0.3\n",
    "IOOt = 0.7\n",
    "\n",
    "filtered_boxes = truncated_nms(predictions, IOUt, IOIt, IOOt)\n",
    "print(filtered_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce50f18c-291c-48c5-b24d-e9233c579bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def truncated_nms_2(\n",
    "    predictions: torch.Tensor,\n",
    "    match_metric: str = \"IOU\",\n",
    "    IOUt: float = 0.5,  # IoU threshold for suppression\n",
    "    IOIt: float = 0.5,  # Intersection over Inside threshold\n",
    "    IOOt: float = 0.5,  # Intersection over Outside threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform Truncated Non-Maximum Suppression (NMS).\n",
    "    \n",
    "    Args:\n",
    "        predictions: (tensor) The location preds for the image\n",
    "            along with the class predscores, Shape: [num_boxes, 5].\n",
    "        match_metric: (str) 'IOU' or 'IOS'\n",
    "        IOUt: (float) IoU threshold for the final decision.\n",
    "        IOIt: (float) Intersection over Inside Box threshold.\n",
    "        IOOt: (float) Intersection over Outside Box threshold.\n",
    "\n",
    "    Returns:\n",
    "        A list of filtered indexes, Shape: [ ,]\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract coordinates for each prediction box\n",
    "    x1 = predictions[:, 0]\n",
    "    y1 = predictions[:, 1]\n",
    "    x2 = predictions[:, 2]\n",
    "    y2 = predictions[:, 3]\n",
    "\n",
    "    # Extract the confidence scores as well\n",
    "    scores = predictions[:, 4]\n",
    "\n",
    "    # Calculate the area of each bounding box\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    \n",
    "    # Sort the prediction boxes based on their confidence scores\n",
    "    order = scores.argsort()\n",
    "\n",
    "    # Initialize an empty list for filtered prediction boxes\n",
    "    keep = []\n",
    "\n",
    "    while len(order) > 0:\n",
    "        # Extract the index of the prediction with the highest score\n",
    "        idx = order[-1]\n",
    "\n",
    "        # Add this prediction to the filtered list\n",
    "        keep.append(idx.item())  # Use .item() to append the value directly\n",
    "\n",
    "        # Remove the selected box from further consideration\n",
    "        order = order[:-1]\n",
    "\n",
    "        # Sanity check: if there are no boxes left to process, break out\n",
    "        if len(order) == 0:\n",
    "            break\n",
    "\n",
    "        # Select coordinates of the remaining boxes based on 'order'\n",
    "        xx1 = x1[order]\n",
    "        yy1 = y1[order]\n",
    "        xx2 = x2[order]\n",
    "        yy2 = y2[order]\n",
    "\n",
    "        # Find the coordinates of the intersection boxes\n",
    "        xx1 = torch.max(xx1, x1[idx])\n",
    "        yy1 = torch.max(yy1, y1[idx])\n",
    "        xx2 = torch.min(xx2, x2[idx])\n",
    "        yy2 = torch.min(yy2, y2[idx])\n",
    "\n",
    "        # Find the width and height of the intersection boxes\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "\n",
    "        # Clamp to avoid negative width/height (non-overlapping boxes)\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "\n",
    "        # Calculate the intersection area\n",
    "        inter = w * h\n",
    "\n",
    "        # Calculate the areas of the remaining boxes\n",
    "        rem_areas = areas[order]\n",
    "\n",
    "        # Calculate the Intersection over Union (IoU) or Intersection over Smaller Box (IoS)\n",
    "        if match_metric == \"IOU\":\n",
    "            # Calculate the union of every prediction and the selected prediction\n",
    "            union = (rem_areas - inter) + areas[idx]\n",
    "            # Compute the IoU of each prediction with the selected one\n",
    "            match_metric_value = inter / union\n",
    "\n",
    "        elif match_metric == \"IOS\":\n",
    "            # Calculate the smaller area between the current and selected boxes\n",
    "            smaller = torch.min(rem_areas, areas[idx])\n",
    "            # Compute the IoS of each prediction with the selected one\n",
    "            match_metric_value = inter / smaller\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown match metric: {match_metric}\")\n",
    "\n",
    "        # Calculate the intersection areas relative to the box S (idx)\n",
    "        inter_area_M = inter / ((x2[idx] - x1[idx]) * (y2[idx] - y1[idx]))  # Relative to box M\n",
    "        inter_area_bi = inter / ((xx2 - xx1) * (yy2 - yy1))  # Relative to box bi\n",
    "        \n",
    "        # Apply conditions based on the truncated NMS logic\n",
    "        condition1 = (inter_area_M > IOIt) and (inter_area_bi <= IOOt)\n",
    "        condition2 = (inter_area_M <= IOOt) and (inter_area_bi > IOIt)\n",
    "        condition = condition1 or condition2\n",
    "\n",
    "        if not condition1.any():\n",
    "            keep.append(idx.item())\n",
    "        \n",
    "        # Keep boxes that have IoU/Ios lower than the threshold and satisfy the conditions\n",
    "        mask = (match_metric_value < IOUt) & ~condition\n",
    "        order = order[mask]\n",
    "\n",
    "    print(\"Total valid predictions: \", len(keep))\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2de2e266-4dd7-4d49-8ae8-0405e3f9ab3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (704029454.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[56], line 38\u001b[0;36m\u001b[0m\n\u001b[0;31m    B = predictions[, 0:4]\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate IoU (Intersection over Union)\n",
    "def iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union (IoU) between two bounding boxes.\n",
    "    Assumes boxes are in the format [x1, y1, x2, y2].\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    return intersection_area / union_area if union_area > 0 else 0\n",
    "\n",
    "# Truncated NMS Implementation\n",
    "def truncated_nms_lt(predictions:torch.tensor, IOUt, IOIt, IOOt):\n",
    "    \"\"\"\n",
    "    Perform Truncated Non-Maximum Suppression.\n",
    "    \n",
    "    Parameters:\n",
    "    B - List of bounding boxes (each box as [x1, y1, x2, y2])\n",
    "    S - List of corresponding confidence scores\n",
    "    IOUt - IoU threshold\n",
    "    IOIt - Inside IoU threshold\n",
    "    IOOt - Outside IoU threshold\n",
    "    \n",
    "    Returns:\n",
    "    K - Filtered list of bounding boxes\n",
    "    \"\"\"\n",
    "    B = predictions[,:4]\n",
    "    S = predictions[, 4]\n",
    "    K = []  # Output list of selected boxes\n",
    "    \n",
    "    while len(B) > 0:\n",
    "        # Find the box with the highest score\n",
    "        m = np.argmax(S)\n",
    "        M = B[m]\n",
    "\n",
    "        # Remove the selected box from B and S\n",
    "        B.pop(m)\n",
    "        S.pop(m)\n",
    "\n",
    "        # Check remaining boxes\n",
    "        new_B = []\n",
    "        new_S = []\n",
    "        \n",
    "        for i in range(len(B)):\n",
    "            bi = B[i]\n",
    "            iou_M_bi = iou(M, bi)\n",
    "\n",
    "            # Condition 1: M > IOIt and IoU(M, bi) <= IOOt\n",
    "            condition1 = (iou_M_bi > IOIt) and (iou_M_bi <= IOOt)\n",
    "\n",
    "            # Condition 2: M <= IOOt and IoU(M, bi) > IOIt\n",
    "            condition2 = (iou_M_bi <= IOOt) and (iou_M_bi > IOIt)\n",
    "\n",
    "            if not condition1:\n",
    "                K.append(M)\n",
    "\n",
    "            # If IoU is within threshold, remove bi; otherwise, keep it\n",
    "            if not ((iou_M_bi <= IOUt) and not (condition1 or condition2)):\n",
    "                new_B.append(bi)\n",
    "                new_S.append(S[i])\n",
    "\n",
    "        B = new_B\n",
    "        S = new_S\n",
    "\n",
    "    return K\n",
    "\n",
    "# Example usage\n",
    "predictions = torch.tensor([\n",
    "    [10, 10, 50, 50, 0.9, 1],\n",
    "    [12, 12, 48, 48, 0.85, 1],\n",
    "    [60, 60, 100, 100, 0.8, 2]\n",
    "])\n",
    "\n",
    "IOUt = 0.7\n",
    "IOIt = 0.5\n",
    "IOOt = 0.3\n",
    "\n",
    "result = truncated_nms_lt(predictions, IOUt, IOIt, IOOt)\n",
    "print(\"Filtered Boxes:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6cea77-4b6b-4a99-aeb7-5c4d9a52cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def truncated_nms_merge(\n",
    "    predictions: torch.Tensor,\n",
    "    match_metric: str = \"IOU\",\n",
    "    IOUt: float = 0.7,  # Truncation IoU threshold for keeping boxes\n",
    "    IOIt: float = 0.5,  # Inside IoU threshold\n",
    "    IOOt: float = 0.3   # Outside IoU threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply truncated non-maximum suppression to avoid detecting too many\n",
    "    overlapping bounding boxes for a given object, with added truncation logic.\n",
    "\n",
    "    Args:\n",
    "        predictions (tensor): The location preds for the image along with the class scores, Shape: [num_boxes, 5].\n",
    "        match_metric (str): IOU or IOS (Intersection over Area or Intersection over Union)\n",
    "        match_threshold (float): The overlap threshold for match metric.\n",
    "        IOUt (float): Intersection over Union threshold for truncation (threshold to keep boxes)\n",
    "        IOIt (float): Inside Intersection over Union threshold (threshold to keep inside box)\n",
    "        IOOt (float): Outside Intersection over Union threshold (threshold for outside box)\n",
    "    \n",
    "    Returns:\n",
    "        List: A list of filtered indexes\n",
    "    \"\"\"\n",
    "    # Extract coordinates for every prediction box present in P\n",
    "    x1 = predictions[:, 0]\n",
    "    y1 = predictions[:, 1]\n",
    "    x2 = predictions[:, 2]\n",
    "    y2 = predictions[:, 3]\n",
    "\n",
    "    # Extract the confidence scores\n",
    "    scores = predictions[:, 4]\n",
    "\n",
    "    # Calculate area of every box\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # Sort the prediction boxes in P according to their confidence scores\n",
    "    order = scores.argsort()\n",
    "\n",
    "    # Initialize an empty list for filtered prediction boxes\n",
    "    keep = []\n",
    "\n",
    "    while len(order) > 0:\n",
    "        # Extract the index of the prediction with the highest score (S)\n",
    "        idx = order[-1]\n",
    "\n",
    "        # Push S in filtered predictions list\n",
    "        keep.append(idx.tolist())\n",
    "\n",
    "        # Remove S from P\n",
    "        order = order[:-1]\n",
    "\n",
    "        # Sanity check\n",
    "        if len(order) == 0:\n",
    "            break\n",
    "\n",
    "        # Select coordinates of remaining boxes according to the indices in order\n",
    "        xx1 = torch.index_select(x1, dim=0, index=order)\n",
    "        xx2 = torch.index_select(x2, dim=0, index=order)\n",
    "        yy1 = torch.index_select(y1, dim=0, index=order)\n",
    "        yy2 = torch.index_select(y2, dim=0, index=order)\n",
    "\n",
    "        # Find the coordinates of the intersection boxes\n",
    "        xx1 = torch.max(xx1, x1[idx])\n",
    "        yy1 = torch.max(yy1, y1[idx])\n",
    "        xx2 = torch.min(xx2, x2[idx])\n",
    "        yy2 = torch.min(yy2, y2[idx])\n",
    "\n",
    "        # Find height and width of the intersection boxes\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "\n",
    "        # Take max with 0.0 to avoid negative width and height\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "\n",
    "        # Find the intersection area\n",
    "        inter = w * h\n",
    "\n",
    "        # Find the areas of the remaining boxes according to the indices in order\n",
    "        rem_areas = torch.index_select(areas, dim=0, index=order)\n",
    "\n",
    "        # Calculate the match metric value (IoU or IoS)\n",
    "        if match_metric == \"IOU\":\n",
    "            # Find the union of every prediction T in P with the prediction S\n",
    "            union = (rem_areas - inter) + areas[idx]\n",
    "            match_metric_value = inter / union\n",
    "        elif match_metric == \"IOS\":\n",
    "            # Find the smaller area of every prediction T in P with the prediction S\n",
    "            smaller = torch.min(rem_areas, areas[idx])\n",
    "            match_metric_value = inter / smaller\n",
    "        else:\n",
    "            raise ValueError(\"Invalid match_metric. Choose either 'IOU' or 'IOS'.\")\n",
    "\n",
    "        # Add the condition for Truncated NMS:\n",
    "        # - Keep boxes with IoU below the truncation threshold (IOUt)\n",
    "        # - Handle inside (IOIt) and outside (IOOt) intersection thresholds\n",
    "        mask = (match_metric_value < IOUt)  # Keep boxes with IoU below threshold\n",
    "\n",
    "        # Apply the truncated NMS conditions\n",
    "        for i, m in enumerate(mask):\n",
    "            # Condition for truncated NMS (condition 1 and 2 from original pseudocode)\n",
    "            iou = match_metric_value[i]\n",
    "            if iou > IOIt and iou <= IOOt:  # Inside box condition\n",
    "                mask[i] = False  # Remove box if condition is met\n",
    "            elif iou <= IOOt and iou > IOIt:  # Outside box condition\n",
    "                mask[i] = True  # Keep box if condition is met\n",
    "\n",
    "        # Filter out the boxes based on the updated mask\n",
    "        order = order[mask]\n",
    "\n",
    "    print(\"Total Valid prediction: \", len(keep))\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99de20c5-d110-422a-b080-0182ff413b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Valid prediction:  2\n",
      "Filtered Boxes: [0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "predictions = torch.tensor([\n",
    "    [10, 10, 50, 50, 0.9, 1],\n",
    "    [12, 12, 48, 48, 0.85, 1],\n",
    "    [60, 60, 100, 100, 0.8, 2]\n",
    "])\n",
    "\n",
    "IOUt = 0.7\n",
    "IOIt = 0.5\n",
    "IOOt = 0.3\n",
    "match_threshold = 0.5\n",
    "match_metric= 'IOU'\n",
    "\n",
    "result = truncated_nms_merge(predictions,match_metric, match_threshold, IOUt, IOIt, IOOt)\n",
    "print(\"Filtered Boxes:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23da2f7c-b54d-4429-aa43-b90a2091ddd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Valid prediction:  2\n",
      "Filtered Boxes: [0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "predictions = torch.tensor([\n",
    "    [10, 10, 50, 50, 0.9, 1],\n",
    "    [12, 12, 48, 48, 0.85, 1],\n",
    "    [60, 60, 100, 100, 0.8, 2]\n",
    "])\n",
    "\n",
    "IOUt = 0.7\n",
    "IOIt = 0.5\n",
    "IOOt = 0.3\n",
    "match_threshold = 0.5\n",
    "match_metric= 'IOU'\n",
    "\n",
    "result = truncated_nms_merge(predictions, match_metric, IOUt, IOIt, IOOt)\n",
    "print(\"Filtered Boxes:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fc15ec-ead2-43cf-a643-81c0a6fea0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: person, Score: 0.60, BBox: (30,430,100,470)\n",
      "Label: car, Score: 0.80, BBox: (55,25,155,125)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "# --- Reuse DetectionBox and all filter/NMS functions from earlier ---\n",
    "\n",
    "class DetectionBox:\n",
    "    def __init__(self, x1, y1, x2, y2, score, label):\n",
    "        self.x1 = x1; self.y1 = y1; self.x2 = x2; self.y2 = y2\n",
    "        self.score = score; self.label = label\n",
    "\n",
    "    def get_center(self):\n",
    "        return ((self.x1 + self.x2)/2, (self.y1 + self.y2)/2)\n",
    "    \n",
    "    def get_area(self):\n",
    "        return (self.x2 - self.x1) * (self.y2 - self.y1)\n",
    "    \n",
    "    def is_touching_edge(self, img_w, img_h, edge_thresh=10):\n",
    "        return (\n",
    "            abs(self.x1) <= edge_thresh or\n",
    "            abs(self.y1) <= edge_thresh or\n",
    "            abs(img_w - self.x2) <= edge_thresh or\n",
    "            abs(img_h - self.y2) <= edge_thresh\n",
    "        )\n",
    "\n",
    "def contextual_filter(boxes: List[DetectionBox], img_h: int) -> List[bool]:\n",
    "    return [\n",
    "        not (b.label in ['car','bus','truck'] and b.get_center()[1] < img_h * 0.3)\n",
    "        for b in boxes\n",
    "    ]\n",
    "\n",
    "def correct_misclassifications(boxes: List[DetectionBox], confusion_map: Dict[str, str]) -> List[DetectionBox]:\n",
    "    return [\n",
    "        DetectionBox(b.x1, b.y1, b.x2, b.y2, b.score, confusion_map.get(b.label, b.label))\n",
    "        for b in boxes\n",
    "    ]\n",
    "\n",
    "def boundary_filter(boxes: List[DetectionBox], img_w: int, img_h: int) -> List[bool]:\n",
    "    return [\n",
    "        not b.is_touching_edge(img_w, img_h) or b.score >= 0.5\n",
    "        for b in boxes\n",
    "    ]\n",
    "\n",
    "def final_score_adjustment(\n",
    "    boxes: List[DetectionBox],\n",
    "    region_mask: List[bool],\n",
    "    boundary_mask: List[bool],\n",
    "    min_score: float = 0.3\n",
    ") -> List[DetectionBox]:\n",
    "    adjusted = []\n",
    "    for b, r_valid, e_valid in zip(boxes, region_mask, boundary_mask):\n",
    "        score = b.score * (0.5 if not r_valid else 1.0) * (0.7 if not e_valid else 1.0)\n",
    "        if score >= min_score:\n",
    "            adjusted.append(DetectionBox(b.x1, b.y1, b.x2, b.y2, score, b.label))\n",
    "    return adjusted\n",
    "\n",
    "# Pure-Python IoU and NMS\n",
    "def iou(box1: DetectionBox, box2: DetectionBox) -> float:\n",
    "    ix1 = max(box1.x1, box2.x1); iy1 = max(box1.y1, box2.y1)\n",
    "    ix2 = min(box1.x2, box2.x2); iy2 = min(box1.y2, box2.y2)\n",
    "    iw = max(0, ix2 - ix1); ih = max(0, iy2 - iy1)\n",
    "    inter = iw * ih\n",
    "    if inter == 0: return 0.0\n",
    "    union = box1.get_area() + box2.get_area() - inter\n",
    "    return inter / union\n",
    "\n",
    "def apply_nms(boxes: List[DetectionBox], iou_thresh: float = 0.5) -> List[DetectionBox]:\n",
    "    final = []\n",
    "    for cls in set(b.label for b in boxes):\n",
    "        cls_boxes = [b for b in boxes if b.label == cls]\n",
    "        cls_boxes.sort(key=lambda b: b.score, reverse=True)\n",
    "        keep = []\n",
    "        while cls_boxes:\n",
    "            current = cls_boxes.pop(0)\n",
    "            keep.append(current)\n",
    "            cls_boxes = [b for b in cls_boxes if iou(current, b) < iou_thresh]\n",
    "        final.extend(keep)\n",
    "    return final\n",
    "\n",
    "# --- Integrated Pipeline ---\n",
    "\n",
    "def filter_and_nms(\n",
    "    detections: List[DetectionBox],\n",
    "    image_width: int,\n",
    "    image_height: int,\n",
    "    confusion_map: Dict[str, str],\n",
    "    iou_threshold: float = 0.5,\n",
    "    min_score: float = 0.3\n",
    ") -> List[DetectionBox]:\n",
    "    # Step 3: Contextual filtering\n",
    "    region_mask = contextual_filter(detections, image_height)\n",
    "    # Step 6: Label correction\n",
    "    corrected = correct_misclassifications(detections, confusion_map)\n",
    "    # Step 8: Boundary filtering\n",
    "    boundary_mask = boundary_filter(corrected, image_width, image_height)\n",
    "    # Step 9: Score adjustment & thresholding\n",
    "    adjusted = final_score_adjustment(corrected, region_mask, boundary_mask, min_score)\n",
    "    # NMS\n",
    "    return apply_nms(adjusted, iou_threshold)\n",
    "\n",
    "\n",
    "# ----- Example Usage -----\n",
    "if __name__ == \"__main__\":\n",
    "    img_w, img_h = 640, 480\n",
    "    raw_preds = [\n",
    "        DetectionBox(50,20,150,120,0.9,'car'),\n",
    "        DetectionBox(30,430,100,470,0.6,'person'),\n",
    "        DetectionBox(600,10,639,100,0.4,'bus'),\n",
    "        DetectionBox(55,25,155,125,0.8,'van')\n",
    "    ]\n",
    "    confusion = {'van':'car','truck':'bus'}\n",
    "\n",
    "    final_detections = filter_and_nms(\n",
    "        raw_preds, img_w, img_h, confusion,\n",
    "        iou_threshold=0.5, min_score=0.3\n",
    "    )\n",
    "    # Print results\n",
    "    for det in final_detections:\n",
    "        print(f\"Label: {det.label}, Score: {det.score:.2f}, BBox: ({det.x1},{det.y1},{det.x2},{det.y2})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f19575-320d-4a06-a8e9-fbccf811aab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person: 0.60, 30,430,100,470\n",
      "van: 0.80, 55,25,155,125\n",
      "car: 0.45, 50,20,150,120\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "# --- DetectionBox and Filters (without confusion map) ---\n",
    "\n",
    "class DetectionBox:\n",
    "    def __init__(self, x1, y1, x2, y2, score, label):\n",
    "        self.x1 = x1; self.y1 = y1; self.x2 = x2; self.y2 = y2\n",
    "        self.score = score; self.label = label\n",
    "\n",
    "    def get_center(self):\n",
    "        return ((self.x1 + self.x2) / 2, (self.y1 + self.y2) / 2)\n",
    "    \n",
    "    def get_area(self):\n",
    "        return (self.x2 - self.x1) * (self.y2 - self.y1)\n",
    "    \n",
    "    def is_touching_edge(self, img_w, img_h, edge_thresh=10):\n",
    "        return (\n",
    "            abs(self.x1) <= edge_thresh or\n",
    "            abs(self.y1) <= edge_thresh or\n",
    "            abs(img_w - self.x2) <= edge_thresh or\n",
    "            abs(img_h - self.y2) <= edge_thresh\n",
    "        )\n",
    "\n",
    "def contextual_filter(boxes: List[DetectionBox], img_h: int) -> List[bool]:\n",
    "    return [\n",
    "        not (b.label in ['car', 'bus', 'truck'] and b.get_center()[1] < img_h * 0.3)\n",
    "        for b in boxes\n",
    "    ]\n",
    "\n",
    "def boundary_filter(boxes: List[DetectionBox], img_w: int, img_h: int) -> List[bool]:\n",
    "    return [\n",
    "        not b.is_touching_edge(img_w, img_h) or b.score >= 0.5\n",
    "        for b in boxes\n",
    "    ]\n",
    "\n",
    "def final_score_adjustment(\n",
    "    boxes: List[DetectionBox],\n",
    "    region_mask: List[bool],\n",
    "    boundary_mask: List[bool],\n",
    "    min_score: float = 0.3\n",
    ") -> List[DetectionBox]:\n",
    "    adjusted = []\n",
    "    for b, r_valid, e_valid in zip(boxes, region_mask, boundary_mask):\n",
    "        score = b.score * (0.5 if not r_valid else 1.0) * (0.7 if not e_valid else 1.0)\n",
    "        if score >= min_score:\n",
    "            adjusted.append(DetectionBox(b.x1, b.y1, b.x2, b.y2, score, b.label))\n",
    "    return adjusted\n",
    "\n",
    "# Pure-Python IoU and NMS\n",
    "def iou(box1: DetectionBox, box2: DetectionBox) -> float:\n",
    "    ix1 = max(box1.x1, box2.x1); iy1 = max(box1.y1, box2.y1)\n",
    "    ix2 = min(box1.x2, box2.x2); iy2 = min(box1.y2, box2.y2)\n",
    "    iw = max(0, ix2 - ix1); ih = max(0, iy2 - iy1)\n",
    "    inter = iw * ih\n",
    "    if inter == 0: return 0.0\n",
    "    union = box1.get_area() + box2.get_area() - inter\n",
    "    return inter / union\n",
    "\n",
    "def apply_nms(boxes: List[DetectionBox], iou_thresh: float = 0.5) -> List[DetectionBox]:\n",
    "    final = []\n",
    "    for cls in set(b.label for b in boxes):\n",
    "        cls_boxes = [b for b in boxes if b.label == cls]\n",
    "        cls_boxes.sort(key=lambda b: b.score, reverse=True)\n",
    "        keep = []\n",
    "        while cls_boxes:\n",
    "            curr = cls_boxes.pop(0)\n",
    "            keep.append(curr)\n",
    "            cls_boxes = [b for b in cls_boxes if iou(curr, b) < iou_thresh]\n",
    "        final.extend(keep)\n",
    "    return final\n",
    "\n",
    "# --- Integrated Pipeline (without confusion map) ---\n",
    "def filter_and_nms(\n",
    "    detections: List[DetectionBox],\n",
    "    image_width: int,\n",
    "    image_height: int,\n",
    "    iou_threshold: float = 0.5,\n",
    "    min_score: float = 0.3\n",
    ") -> List[DetectionBox]:\n",
    "    region_mask = contextual_filter(detections, image_height)\n",
    "    boundary_mask = boundary_filter(detections, image_width, image_height)\n",
    "    adjusted = final_score_adjustment(detections, region_mask, boundary_mask, min_score)\n",
    "    return apply_nms(adjusted, iou_threshold)\n",
    "\n",
    "# ----- Example Usage -----\n",
    "if __name__ == \"__main__\":\n",
    "    img_w, img_h = 640, 480\n",
    "    raw_preds = [\n",
    "        DetectionBox(50, 20, 150, 120, 0.9, 'car'),\n",
    "        DetectionBox(30, 430, 100, 470, 0.6, 'person'),\n",
    "        DetectionBox(600, 10, 639, 100, 0.4, 'bus'),\n",
    "        DetectionBox(55, 25, 155, 125, 0.8, 'van')\n",
    "    ]\n",
    "\n",
    "    final_detections = filter_and_nms(\n",
    "        raw_preds, img_w, img_h,\n",
    "        iou_threshold=0.5, min_score=0.3\n",
    "    )\n",
    "    for d in final_detections:\n",
    "        print(f\"{d.label}: {d.score:.2f}, {d.x1},{d.y1},{d.x2},{d.y2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch11.8]",
   "language": "python",
   "name": "conda-env-torch11.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
