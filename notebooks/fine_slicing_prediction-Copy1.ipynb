{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a15879f8-e7c4-45bf-96a2-4654b2365ebd",
   "metadata": {},
   "source": [
    "#### **Import Libraries and Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be129749-5a0b-4e23-a14d-053c688f754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmfs1/cm/shared/apps_local/python/3.11/envs/torch11.8/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required functions and classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image, read_image_as_pil\n",
    "from sahi.utils.file import Path, increment_path, list_files, save_json, save_pickle, download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict, agg_prediction, get_prediction_batched, get_sliced_prediction_batched, predict \n",
    "from sahi.prediction import visualize_object_predictions\n",
    "from IPython.display import Image\n",
    "from numpy import asarray\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sahi.prediction import ObjectPrediction, PredictionResult\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import torch\n",
    "from torchvision.ops import nms, clip_boxes_to_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3109b6-aa06-427e-8647-c8e440fc40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download YOLOv8-S model to 'models/yolov8s.pt'\n",
    "yolov8_model_path = 'models/yolov8/last.pt'\n",
    "#download_yolov8s_model(destination_path=yolov8_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a18775-2cab-461c-b7ec-a691e66dfa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=yolov8_model_path,\n",
    "    confidence_threshold=0.3,\n",
    "    device=\"cuda:0\", # or 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c67d29b-325c-470c-98c5-2e2cb121c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slice_parameters(object_density, slice_size):\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    #image_path = \"test_data/0000006_06773_d_0000018.jpg\"\n",
    "    #image = Image.open(image_path).convert(\"RGB\")\n",
    "    #image_width, image_height  = image.size\n",
    "    #print(\"Image Width:\", image_width)\n",
    "    #print(\"Image Height:\", image_height)\n",
    "    #min_dim = min(image_width, image_height)\n",
    "    #slice_size = min_dim // 4 if min_dim > 1600 else min_dim // 2\n",
    "    #print(f\"Dimension calculation time taken: {(time.time() - start_time)*1000:.2f} ms\")\n",
    "\n",
    "    \n",
    "    if object_density >= 50:\n",
    "        #slice_size = min_dim // 4\n",
    "        slice_width = slice_size\n",
    "        slice_height = slice_size\n",
    "        overlap_width_ratio = 0.5\n",
    "        overlap_height_ratio = 0.5\n",
    "    elif 25 <= object_density < 50:\n",
    "        #slice_size = min_dim // 2\n",
    "        slice_width = slice_size\n",
    "        slice_height = slice_size\n",
    "        overlap_width_ratio = 0.25\n",
    "        overlap_height_ratio = 0.25\n",
    "    elif 10 <= object_density < 25:\n",
    "        #slice_size = min_dim // 2\n",
    "        slice_width = slice_size\n",
    "        slice_height = slice_size\n",
    "        overlap_width_ratio = 0.15\n",
    "        overlap_height_ratio = 0.15\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    return slice_width, slice_height, overlap_width_ratio, overlap_height_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eba1a1e0-6946-4db9-8a58-19442d6a4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_merge(predictions, iou_threshold=0.5):\n",
    "    if not predictions:\n",
    "        return []\n",
    "    print(\"IOU Threshold: \", iou_threshold)\n",
    "    boxes = torch.tensor([p.bbox.to_xyxy() for p in predictions])\n",
    "    scores = torch.tensor([p.score.value for p in predictions])\n",
    "    keep_indices = nms(boxes, scores, iou_threshold).tolist()\n",
    "    #keep_indices = clip_boxes_to_image(boxes, 32).tolist()\n",
    "    return [predictions[i] for i in keep_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c7e3ccd-81b0-4357-a91e-ca8f13963f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fine_sliced_images(input_folder, dataset_json_path, detection_model, base_slice_size=512):\n",
    "    name = \"exp\"\n",
    "    save_dir = Path(increment_path(Path(\"sliced_predictions\") / name, exist_ok=False))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    data = {}\n",
    "    if dataset_json_path:\n",
    "        with open(dataset_json_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "    vis_params = {\n",
    "        \"bbox_thickness\": 2,\n",
    "        \"text_size\": 0.5,\n",
    "        \"text_thickness\": 1,\n",
    "        \"hide_labels\": False,\n",
    "        \"hide_conf\": False,\n",
    "        \"format\": \"png\"\n",
    "    }\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nðŸš€ Running fine slicing prediction on {len(image_files)} images...\")\n",
    "\n",
    "    all_coco_preds = []\n",
    "\n",
    "    for filename in image_files:\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image_pil)\n",
    "        image_h, image_w = image_np.shape[:2]\n",
    "        filename_wo_ext = Path(filename).stem\n",
    "        img_id = next((img[\"id\"] for img in data.get(\"images\", []) if img[\"file_name\"].startswith(filename_wo_ext)), None)\n",
    "\n",
    "        print(\"*****************************************\")\n",
    "        print(\"File Name\", filename_wo_ext)\n",
    "        \n",
    "        all_object_predictions = []\n",
    "\n",
    "        # Split image into 2x2 grid\n",
    "        grid_h, grid_w = image_h // 2, image_w // 2\n",
    "\n",
    "        for row in range(2):\n",
    "            for col in range(2):\n",
    "                x1, y1 = col * grid_w, row * grid_h\n",
    "                x2, y2 = min(x1 + grid_w, image_w), min(y1 + grid_h, image_h)\n",
    "                sub_img = image_pil.crop((x1, y1, x2, y2))\n",
    "                print(\"Cropped Image:\", x1, y1, x2, y2)\n",
    "                # Base prediction on the sub-image\n",
    "                base_pred = get_prediction(sub_img, detection_model)\n",
    "                object_density = len(base_pred.object_prediction_list)\n",
    "                print(\"Object Density:\", object_density)\n",
    "                \n",
    "                slice_params = get_slice_parameters(object_density, base_slice_size)\n",
    "\n",
    "                if slice_params:\n",
    "                    slice_width, slice_height, overlap_w, overlap_h = slice_params\n",
    "\n",
    "                    print(\"********* Slice Parameters ***********\")\n",
    "                    print(\"Slice Width: \", slice_width)\n",
    "                    print(\"Slice Height: \", slice_height)\n",
    "                    print(\"Overlap Width Ratio: \", overlap_w)\n",
    "                    print(\"Overlap Height Ratio: \", overlap_h)\n",
    "                    \n",
    "                    sliced_pred = get_sliced_prediction(\n",
    "                        sub_img,\n",
    "                        detection_model,\n",
    "                        slice_height=slice_height,\n",
    "                        slice_width=slice_width,\n",
    "                        overlap_height_ratio=overlap_h,\n",
    "                        overlap_width_ratio=overlap_w,\n",
    "                        postprocess_type=\"OptNMS\",\n",
    "                        postprocess_match_metric=\"IOU\",\n",
    "                        postprocess_match_threshold=0.3,\n",
    "                        postprocess_min_area=16,\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    preds = sliced_pred.object_prediction_list\n",
    "                    print(\"Sliced Level Prediction Count: \", len(preds))\n",
    "                else:\n",
    "                    preds = base_pred.object_prediction_list\n",
    "                    print(\"Sliced Level Prediction Count: \", len(preds))\n",
    "\n",
    "                all_object_predictions = preds\n",
    "                # Offset predictions to original image coordinates\n",
    "                #for pred in preds:\n",
    "                 #   pred.shift_amount(x_offset=x1, y_offset=y1)\n",
    "                 #   all_object_predictions.append(pred)\n",
    "\n",
    "        # Apply NMS to merged predictions\n",
    "        merged_preds = nms_merge(all_object_predictions, iou_threshold=0.3)\n",
    "\n",
    "        # Visualization\n",
    "        visualize_object_predictions(\n",
    "            image=np.ascontiguousarray(image_pil),\n",
    "            object_prediction_list=preds,\n",
    "            rect_th=vis_params[\"bbox_thickness\"],\n",
    "            text_size=vis_params[\"text_size\"],\n",
    "            text_th=vis_params[\"text_thickness\"],\n",
    "            hide_labels=vis_params[\"hide_labels\"],\n",
    "            hide_conf=vis_params[\"hide_conf\"],\n",
    "            output_dir=save_dir,\n",
    "            file_name=filename_wo_ext,\n",
    "            export_format=vis_params[\"format\"]\n",
    "        )\n",
    "\n",
    "        # COCO conversion\n",
    "        #coco_preds = [p.to_coco_predictions(image_id=img_id) for p in merged_preds]\n",
    "        #all_coco_preds.extend(coco_preds)\n",
    "\n",
    "    #if dataset_json_path:\n",
    "     #   save_json(all_coco_preds, str(save_dir / \"result.json\"))\n",
    "      #  print(f\"\\n Saved COCO results to {save_dir / 'result.json'}\")\n",
    "\n",
    "    print(f\"\\nâœ… Completed {len(image_files)} images.\")\n",
    "    print(f\"Prediction results are successfully exported to {save_dir}\")\n",
    "    return all_coco_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a965e4a8-6d81-4f1c-ba44-d7f629e4dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fine_sliced_images(input_folder, dataset_json_path, detection_model, base_slice_size=512):\n",
    "    name = \"exp\"\n",
    "    save_dir = Path(increment_path(Path(\"sliced_predictions\") / name, exist_ok=False))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    vis_params = {\n",
    "        \"bbox_thickness\": 2,\n",
    "        \"text_size\": 0.5,\n",
    "        \"text_thickness\": 1,\n",
    "        \"hide_labels\": False,\n",
    "        \"hide_conf\": False,\n",
    "        \"format\": \"png\"\n",
    "    }\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nðŸš€ Running fine slicing prediction on {len(image_files)} images...\")\n",
    "    total_prediction_time = 0.0\n",
    "\n",
    "    for filename in image_files:\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image_pil)\n",
    "        image_h, image_w = image_np.shape[:2]\n",
    "        filename_wo_ext = Path(filename).stem\n",
    "        total_prediction_count = 0\n",
    "        print(\"*****************************************\")\n",
    "        print(\"File Name\", filename_wo_ext)\n",
    "\n",
    "        all_object_predictions = []\n",
    "        # Split image into 2x2 grid\n",
    "        grid_h, grid_w = image_h // 2, image_w // 2\n",
    "\n",
    "        for row in range(2):\n",
    "            for col in range(2):\n",
    "                x1, y1 = col * grid_w, row * grid_h\n",
    "                x2, y2 = min(x1 + grid_w, image_w), min(y1 + grid_h, image_h)\n",
    "                sub_img = image_pil.crop((x1, y1, x2, y2))\n",
    "                print(\"Cropped Image:\", x1, y1, x2, y2)\n",
    "\n",
    "                # Get initial predictions from your detection model\n",
    "                time_start = time.time()\n",
    "                # Base prediction on the sub-image\n",
    "                base_pred = get_prediction(sub_img, detection_model)\n",
    "                time_end = time.time() - time_start\n",
    "                print(\"Initial Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "                \n",
    "                object_density = len(base_pred.object_prediction_list)\n",
    "                print(\"Object Density:\", object_density)\n",
    "                \n",
    "                slice_params = get_slice_parameters(object_density, base_slice_size)\n",
    "\n",
    "                # Add initial prediction time to the cumulative total.\n",
    "                iteration_time = time_end\n",
    "                \n",
    "                if slice_params:\n",
    "                    slice_width, slice_height, overlap_w, overlap_h = slice_params\n",
    "\n",
    "                    print(\"********* Slice Parameters ***********\")\n",
    "                    print(\"Slice Width: \", slice_width)\n",
    "                    print(\"Slice Height: \", slice_height)\n",
    "                    print(\"Overlap Width Ratio: \", overlap_w)\n",
    "                    print(\"Overlap Height Ratio: \", overlap_h)\n",
    "\n",
    "                    time_start_slice = time.time()\n",
    "                    sliced_pred = get_sliced_prediction(\n",
    "                        sub_img,\n",
    "                        detection_model,\n",
    "                        slice_height=slice_height,\n",
    "                        slice_width=slice_width,\n",
    "                        overlap_height_ratio=overlap_h,\n",
    "                        overlap_width_ratio=overlap_w,\n",
    "                        postprocess_type=\"OptNMS\",\n",
    "                        postprocess_match_metric=\"IOU\",\n",
    "                        postprocess_match_threshold=0.3,\n",
    "                        postprocess_min_area=16,\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    time_end_slice = time.time() - time_start_slice\n",
    "                    print(\"Sliced Prediction time is: {:.2f} ms\".format(time_end_slice * 1000))\n",
    "                \n",
    "                    # Add sliced prediction time to the current iteration's total.\n",
    "                    iteration_time += time_end_slice\n",
    "                \n",
    "                    preds = sliced_pred.object_prediction_list\n",
    "                    total_prediction_count += len(preds)\n",
    "                    print(\"Sliced Level Prediction Count 1: \", len(preds))\n",
    "                else:\n",
    "                    print(\"Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "                    preds = base_pred.object_prediction_list\n",
    "                    total_prediction_count += len(preds)\n",
    "                    print(\"Sliced Level Prediction Count 2: \", len(preds))\n",
    "\n",
    "                # Offset predictions to original image coordinates\n",
    "                for pred in preds:\n",
    "                    pred.bbox.minx += x1\n",
    "                    pred.bbox.maxx += x1\n",
    "                    pred.bbox.miny += y1\n",
    "                    pred.bbox.maxy += y1\n",
    "                    all_object_predictions.append(pred)\n",
    "    print(\"________________________________________________\")\n",
    "    print(\"Count Prediction:\",len(all_object_predictions))\n",
    "    # Apply NMS to merged predictions\n",
    "    merged_preds = nms_merge(all_object_predictions, iou_threshold=0.3)\n",
    "\n",
    "    # Visualization for the current slice\n",
    "    slice_filename = f\"{filename_wo_ext}_slice_r{row}_c{col}\"\n",
    "    visualize_object_predictions(\n",
    "        image=np.ascontiguousarray(sub_img),\n",
    "        object_prediction_list=merged_preds,\n",
    "        rect_th=vis_params[\"bbox_thickness\"],\n",
    "        text_size=vis_params[\"text_size\"],\n",
    "        text_th=vis_params[\"text_thickness\"],\n",
    "        hide_labels=vis_params[\"hide_labels\"],\n",
    "        hide_conf=vis_params[\"hide_conf\"],\n",
    "        output_dir=save_dir,\n",
    "        file_name=slice_filename,\n",
    "        export_format=vis_params[\"format\"]\n",
    "    )\n",
    "    # Update the overall total prediction time\n",
    "    total_prediction_time += iteration_time\n",
    "    \n",
    "    print(f\"\\n Completed {len(image_files)} images.\")\n",
    "    print(\"Total Prediction Count: \", (total_prediction_count))\n",
    "    print(\"Total Prediction time for all images is: {:.2f} ms\".format(total_prediction_time * 1000))\n",
    "    print(f\"Prediction results are successfully exported to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e50ad137-85c8-43f7-a0bf-74b5ccbb9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fine_sliced_images_correct(input_folder, dataset_json_path, detection_model, base_slice_size=512):\n",
    "    name = \"exp\"\n",
    "    save_dir = Path(increment_path(Path(\"sliced_predictions\") / name, exist_ok=False))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    vis_params = {\n",
    "        \"bbox_thickness\": 2,\n",
    "        \"text_size\": 0.5,\n",
    "        \"text_thickness\": 1,\n",
    "        \"hide_labels\": False,\n",
    "        \"hide_conf\": False,\n",
    "        \"format\": \"png\"\n",
    "    }\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nðŸš€ Running fine slicing prediction on {len(image_files)} images...\")\n",
    "    total_prediction_time = 0.0\n",
    "\n",
    "    for filename in image_files:\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image_pil)\n",
    "        image_h, image_w = image_np.shape[:2]\n",
    "        filename_wo_ext = Path(filename).stem\n",
    "        total_prediction_count = 0\n",
    "        print(\"*****************************************\")\n",
    "        print(\"File Name\", filename_wo_ext)\n",
    "        all_object_predictions = []\n",
    "\n",
    "        # Split image into 2x2 grid\n",
    "        grid_h, grid_w = image_h // 2, image_w // 2\n",
    "\n",
    "        for row in range(2):\n",
    "            for col in range(2):\n",
    "                x1, y1 = col * grid_w, row * grid_h\n",
    "                x2, y2 = min(x1 + grid_w, image_w), min(y1 + grid_h, image_h)\n",
    "                sub_img = image_pil.crop((x1, y1, x2, y2))\n",
    "                print(\"Cropped Image:\", x1, y1, x2, y2)\n",
    "\n",
    "                # Get initial predictions from your detection model\n",
    "                time_start = time.time()\n",
    "                # Base prediction on the sub-image\n",
    "                base_pred = get_prediction(sub_img, detection_model)\n",
    "                time_end = time.time() - time_start\n",
    "                print(\"Initial Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "                \n",
    "                object_density = len(base_pred.object_prediction_list)\n",
    "                print(\"Object Density:\", object_density)\n",
    "                \n",
    "                slice_params = get_slice_parameters(object_density, base_slice_size)\n",
    "\n",
    "                # Add initial prediction time to the cumulative total.\n",
    "                iteration_time = time_end\n",
    "                \n",
    "                if slice_params:\n",
    "                    slice_width, slice_height, overlap_w, overlap_h = slice_params\n",
    "\n",
    "                    print(\"********* Slice Parameters ***********\")\n",
    "                    print(\"Slice Width: \", slice_width)\n",
    "                    print(\"Slice Height: \", slice_height)\n",
    "                    print(\"Overlap Width Ratio: \", overlap_w)\n",
    "                    print(\"Overlap Height Ratio: \", overlap_h)\n",
    "\n",
    "                    time_start_slice = time.time()\n",
    "                    sliced_pred = get_sliced_prediction(\n",
    "                        sub_img,\n",
    "                        detection_model,\n",
    "                        slice_height=slice_height,\n",
    "                        slice_width=slice_width,\n",
    "                        overlap_height_ratio=overlap_h,\n",
    "                        overlap_width_ratio=overlap_w,\n",
    "                        postprocess_type=\"OptNMS\",\n",
    "                        postprocess_match_metric=\"IOU\",\n",
    "                        postprocess_match_threshold=0.3,\n",
    "                        postprocess_min_area=16,\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    time_end_slice = time.time() - time_start_slice\n",
    "                    print(\"Sliced Prediction time is: {:.2f} ms\".format(time_end_slice * 1000))\n",
    "                \n",
    "                    # Add sliced prediction time to the current iteration's total.\n",
    "                    iteration_time += time_end_slice\n",
    "                \n",
    "                    preds = sliced_pred.object_prediction_list\n",
    "                    total_prediction_count += len(preds)\n",
    "                    print(\"Sliced Level Prediction Count 1: \", len(preds))\n",
    "                else:\n",
    "                    print(\"Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "                    preds = base_pred.object_prediction_list\n",
    "                    total_prediction_count += len(preds)\n",
    "                    print(\"Sliced Level Prediction Count 2: \", len(preds))\n",
    "\n",
    "                # Offset predictions to original image coordinates\n",
    "                for pred in preds:\n",
    "                    pred.bbox.minx += x1\n",
    "                    pred.bbox.maxx += x1\n",
    "                    pred.bbox.miny += y1\n",
    "                    pred.bbox.maxy += y1\n",
    "                    all_object_predictions.append(pred)\n",
    "                # Update the overall total prediction time\n",
    "                total_prediction_time += iteration_time\n",
    "\n",
    "        print(\"________________________________________________\")\n",
    "\n",
    "        # Apply NMS to merged predictions\n",
    "        merged_preds = nms_merge(all_object_predictions, iou_threshold=0.3)\n",
    "        print(\"Merge Count Prediction:\",len(merged_preds))\n",
    "                \n",
    "        # Visualization for the current slice\n",
    "        slice_filename = f\"{filename_wo_ext}_slice_r{row}_c{col}\"\n",
    "        visualize_object_predictions(\n",
    "            image=np.ascontiguousarray(sub_img),\n",
    "            object_prediction_list=merged_preds,\n",
    "            rect_th=vis_params[\"bbox_thickness\"],\n",
    "            text_size=vis_params[\"text_size\"],\n",
    "            text_th=vis_params[\"text_thickness\"],\n",
    "            hide_labels=vis_params[\"hide_labels\"],\n",
    "            hide_conf=vis_params[\"hide_conf\"],\n",
    "            output_dir=save_dir,\n",
    "            file_name=slice_filename,\n",
    "            export_format=vis_params[\"format\"]\n",
    "        )\n",
    "\n",
    "    \n",
    "    print(f\"\\nâœ… Completed {len(image_files)} images.\")\n",
    "    print(\"Total Prediction Count: \", (total_prediction_count))\n",
    "    print(\"Total Prediction time for all images is: {:.2f} ms\".format(total_prediction_time * 1000))\n",
    "    print(f\"Prediction results are successfully exported to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1166c521-5bf9-4de1-8c3f-6ac2008e610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get image details by image_id\n",
    "def get_image_id(coco_data, image_name):\n",
    "    for image in coco_data[\"images\"]:\n",
    "        file_name = Path(image['file_name']).stem\n",
    "        if file_name == image_name:\n",
    "            return image['id']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410b0746-eb7d-40e4-a400-9a72c4d0c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_filter_predictions(predictions: torch.tensor, conf_threshold: float = 0.3, min_area: float = 1024,\n",
    "                      iou_min: float =0.4, iou_max: float = 0.8, area_threshold: int = 2020):\n",
    "    \"\"\"\n",
    "    Filters out predictions with confidence scores below conf_threshold or with an area smaller than min_area.\n",
    "    Args:\n",
    "        predictions: tensor of shape [num_boxes, 6] where column 4 is the confidence.\n",
    "        conf_threshold: Minimum confidence score required.\n",
    "        min_area: Minimum area (width * height) required.\n",
    "    Returns:\n",
    "        Filtered predictions tensor.\n",
    "    \"\"\"\n",
    "    scores = predictions[:, 4]\n",
    "    print(\"Confidence Scores: \", conf_threshold)\n",
    "    print(\"Min area Threshold: \", min_area)\n",
    "    \n",
    "    conf_mask = scores >= conf_threshold\n",
    "\n",
    "    x1 = predictions[:, 0]\n",
    "    y1 = predictions[:, 1]\n",
    "    x2 = predictions[:, 2]\n",
    "    y2 = predictions[:, 3]\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    #area_mask = areas >= min_area\n",
    "\n",
    "    #valid_mask = conf_mask\n",
    "    valid_mask = conf_mask #& area_mask\n",
    "    #valid_mask = area_mask\n",
    "    filtered_areas = areas[valid_mask]\n",
    "\n",
    "    adaptive_iou =  torch.where(filtered_areas < area_threshold, iou_max, iou_min)\n",
    "    \n",
    "    return predictions[valid_mask], adaptive_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "906a9c9c-e9ea-4d8c-b0f2-a4296c3d8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_nms(\n",
    "    predictions: torch.Tensor,\n",
    "    adaptive_iou:torch.float32, # Truncation IoU threshold for keeping boxes\n",
    "    match_metric: str = \"IOU\",\n",
    "    IOIt: float = 0.5,  # Inside IoU threshold (IOIt: 0.3â€“1.0; IOOt: 0â€“0.3)\n",
    "    IOOt: float = 0.3   # Outside IoU threshold (Upper bound of IOOt is lower bound of IOIt)\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply truncated non-maximum suppression to avoid detecting too many\n",
    "    overlapping bounding boxes for a given object, with added truncation logic.\n",
    "\n",
    "    Args:\n",
    "        predictions (tensor): The location preds for the image along with the class scores, Shape: [num_boxes, 5].\n",
    "        match_metric (str): IOU or IOS (Intersection over Area or Intersection over Union)\n",
    "        match_threshold (float): The overlap threshold for match metric.\n",
    "        IOUt (float): Intersection over Union threshold for truncation (threshold to keep boxes)\n",
    "        IOIt (float): Inside Intersection over Union threshold (threshold to keep inside box)\n",
    "        IOOt (float): Outside Intersection over Union threshold (threshold for outside box)\n",
    "    \n",
    "    Returns:\n",
    "        List: A list of filtered indexes\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Adaptive Filtered Prediction: \", len(predictions))\n",
    "    \n",
    "    # Extract coordinates for every prediction box present in P\n",
    "    x1 = predictions[:, 0]\n",
    "    y1 = predictions[:, 1]\n",
    "    x2 = predictions[:, 2]\n",
    "    y2 = predictions[:, 3]\n",
    "\n",
    "    # Extract the confidence scores\n",
    "    scores = predictions[:, 4]\n",
    "\n",
    "    # Calculate area of every box\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # Sort the prediction boxes in P according to their confidence scores\n",
    "    order = scores.argsort()\n",
    "\n",
    "    # Initialize an empty list for filtered prediction boxes\n",
    "    keep = []\n",
    "\n",
    "    while len(order) > 0:\n",
    "        # Extract the index of the prediction with the highest score (S)\n",
    "        idx = order[-1]\n",
    "\n",
    "        # Push S in filtered predictions list\n",
    "        keep.append(idx.tolist())\n",
    "\n",
    "        # Remove S from P\n",
    "        order = order[:-1]\n",
    "\n",
    "        # Sanity check\n",
    "        if len(order) == 0:\n",
    "            break\n",
    "\n",
    "        # Select coordinates of remaining boxes according to the indices in order\n",
    "        xx1 = torch.index_select(x1, dim=0, index=order)\n",
    "        xx2 = torch.index_select(x2, dim=0, index=order)\n",
    "        yy1 = torch.index_select(y1, dim=0, index=order)\n",
    "        yy2 = torch.index_select(y2, dim=0, index=order)\n",
    "\n",
    "        # Find the coordinates of the intersection boxes\n",
    "        xx1 = torch.max(xx1, x1[idx])\n",
    "        yy1 = torch.max(yy1, y1[idx])\n",
    "        xx2 = torch.min(xx2, x2[idx])\n",
    "        yy2 = torch.min(yy2, y2[idx])\n",
    "\n",
    "        # Find height and width of the intersection boxes\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "\n",
    "        # Take max with 0.0 to avoid negative width and height\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "\n",
    "        # Find the intersection area\n",
    "        inter = w * h\n",
    "\n",
    "        # Find the areas of the remaining boxes according to the indices in order\n",
    "        rem_areas = torch.index_select(areas, dim=0, index=order)\n",
    "\n",
    "        # Calculate the match metric value (IoU or IoS)\n",
    "        if match_metric == \"IOU\":\n",
    "            # Find the union of every prediction T in P with the prediction S\n",
    "            union = (rem_areas - inter) + areas[idx]\n",
    "            match_metric_value = inter / union\n",
    "        elif match_metric == \"IOS\":\n",
    "            # Find the smaller area of every prediction T in P with the prediction S\n",
    "            smaller = torch.min(rem_areas, areas[idx])\n",
    "            match_metric_value = inter / smaller\n",
    "        else:\n",
    "            raise ValueError(\"Invalid match_metric. Choose either 'IOU' or 'IOS'.\")\n",
    "\n",
    "        # Add the condition for Truncated NMS:\n",
    "        # - Keep boxes with IoU below the truncation threshold (IOUt)\n",
    "        # - Handle inside (IOIt) and outside (IOOt) intersection thresholds\n",
    "        iou_thresholds = torch.index_select(adaptive_iou, 0, order)\n",
    "        \n",
    "        mask = (match_metric_value < iou_thresholds)  # Keep boxes with IoU below threshold\n",
    "\n",
    "        # Apply the truncated NMS conditions\n",
    "        for i, m in enumerate(mask):\n",
    "            # Condition for truncated NMS (condition 1 and 2 from original pseudocode)\n",
    "            iou = match_metric_value[i]\n",
    "            if iou > IOIt and iou <= IOOt:  # Inside box condition\n",
    "                mask[i] = False  # Remove box if condition is met\n",
    "            elif iou <= IOOt and iou > IOIt:  # Outside box condition\n",
    "                mask[i] = True  # Keep box if condition is met\n",
    "\n",
    "        # Filter out the boxes based on the updated mask\n",
    "        order = order[mask]\n",
    "\n",
    "    print(\"Final Bounding Box Count (Truncated NMS): \", len(keep))\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "779b6a07-9f79-40fb-807b-36067dc4af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fine_sliced_images_refactored(input_folder, dataset_json_path, detection_model, base_slice_size=512):\n",
    "    name = \"exp\"\n",
    "    save_dir = Path(increment_path(Path(\"sliced_predictions\") / name, exist_ok=False))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if dataset_json_path:\n",
    "        with open(dataset_json_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "    vis_params = {\n",
    "        \"bbox_thickness\": 2,\n",
    "        \"text_size\": 0.5,\n",
    "        \"text_thickness\": 1,\n",
    "        \"hide_labels\": False,\n",
    "        \"hide_conf\": False,\n",
    "        \"format\": \"png\"\n",
    "    }\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nðŸš€ Running fine slicing prediction on {len(image_files)} images...\")\n",
    "    total_prediction_time = 0.0\n",
    "    grand_total_predictions = 0\n",
    "    all_coco_preds = []\n",
    "    keep = []\n",
    "\n",
    "    for filename in image_files:\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image_pil)\n",
    "        image_h, image_w = image_np.shape[:2]\n",
    "        filename_wo_ext = Path(filename).stem\n",
    "        total_prediction_count = 0\n",
    "        print(\"*****************************************\")\n",
    "        print(\"File Name\", filename_wo_ext)\n",
    "        img_id = get_image_id(data, filename_wo_ext)\n",
    "        all_object_predictions = []\n",
    "\n",
    "        # Split image into 2x2 grid\n",
    "        grid_h, grid_w = image_h // 2, image_w // 2\n",
    "\n",
    "        for row in range(2):\n",
    "            for col in range(2):\n",
    "                x1, y1 = col * grid_w, row * grid_h\n",
    "                x2, y2 = min(x1 + grid_w, image_w), min(y1 + grid_h, image_h)\n",
    "                sub_img = image_pil.crop((x1, y1, x2, y2))\n",
    "                print(\"Cropped Image:\", x1, y1, x2, y2)\n",
    "\n",
    "                time_start = time.time()\n",
    "                base_pred = get_prediction(sub_img, detection_model)\n",
    "                time_end = time.time() - time_start\n",
    "                print(\"Initial Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "\n",
    "                object_density = len(base_pred.object_prediction_list)\n",
    "                print(\"Object Density:\", object_density)\n",
    "\n",
    "                slice_params = get_slice_parameters(object_density, base_slice_size)\n",
    "\n",
    "                iteration_time = time_end\n",
    "\n",
    "                if slice_params:\n",
    "                    slice_width, slice_height, overlap_w, overlap_h = slice_params\n",
    "                    print(\"********* Slice Parameters ***********\")\n",
    "                    print(\"Slice Width: \", slice_width)\n",
    "                    print(\"Slice Height: \", slice_height)\n",
    "                    print(\"Overlap Width Ratio: \", overlap_w)\n",
    "                    print(\"Overlap Height Ratio: \", overlap_h)\n",
    "\n",
    "                    time_start_slice = time.time()\n",
    "                    sliced_pred = get_sliced_prediction(\n",
    "                        sub_img,\n",
    "                        detection_model,\n",
    "                        slice_height=slice_height,\n",
    "                        slice_width=slice_width,\n",
    "                        overlap_height_ratio=overlap_h,\n",
    "                        overlap_width_ratio=overlap_w,\n",
    "                        postprocess_type=\"OptNMS\",\n",
    "                        postprocess_match_metric=\"IOU\",\n",
    "                        postprocess_match_threshold=0.3,\n",
    "                        postprocess_min_area=16,\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    time_end_slice = time.time() - time_start_slice\n",
    "                    print(\"Sliced Prediction time is: {:.2f} ms\".format(time_end_slice * 1000))\n",
    "                    \n",
    "                    iteration_time += time_end_slice\n",
    "                    preds = sliced_pred.object_prediction_list\n",
    "                    coco_prediction = sliced_pred.to_coco_predictions(image_id=img_id)\n",
    "                    for idx, predict in enumerate(coco_prediction):\n",
    "                        if coco_prediction[idx][\"bbox\"]:\n",
    "                            all_coco_preds.append(predict)\n",
    "                else:\n",
    "                    print(\"Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "                    preds = base_pred.object_prediction_list\n",
    "                    print(\"Base Prediction:\" , base_pred)\n",
    "                    coco_prediction = base_pred.to_coco_predictions(image_id=img_id)\n",
    "                    for idx, predict in enumerate(coco_prediction):\n",
    "                        if coco_prediction[idx][\"bbox\"]:\n",
    "                            all_coco_preds.append(predict)\n",
    "                    \n",
    "                for pred in preds:\n",
    "                    print\n",
    "                    pred.bbox.minx += x1\n",
    "                    pred.bbox.maxx += x1\n",
    "                    pred.bbox.miny += y1\n",
    "                    pred.bbox.maxy += y1\n",
    "                    all_object_predictions.append(pred)\n",
    "\n",
    "                total_prediction_count += len(preds)\n",
    "                total_prediction_time += iteration_time\n",
    "\n",
    "        print(\"________________________________________________\")\n",
    "\n",
    "        #merged_preds = nms_merge(all_object_predictions, iou_threshold=0.1)\n",
    "        #print(\"Merged Prediction Count\", len(merged_preds))\n",
    "\n",
    "        #all_coco_preds_merge = []\n",
    "        # COCO conversion\n",
    "        #coco_preds = [p.to_coco_annotations(image_id=img_id) for p in merged_preds]\n",
    "        #all_coco_preds_merge.append(coco_preds)\n",
    "\n",
    "\n",
    "        visualize_object_predictions(\n",
    "            image=np.ascontiguousarray(image_pil),\n",
    "            object_prediction_list=all_object_predictions,\n",
    "            rect_th=vis_params[\"bbox_thickness\"],\n",
    "            text_size=vis_params[\"text_size\"],\n",
    "            text_th=vis_params[\"text_thickness\"],\n",
    "            hide_labels=vis_params[\"hide_labels\"],\n",
    "            hide_conf=vis_params[\"hide_conf\"],\n",
    "            output_dir=save_dir,\n",
    "            file_name=filename_wo_ext,\n",
    "            export_format=vis_params[\"format\"]\n",
    "        )\n",
    "\n",
    "        grand_total_predictions += total_prediction_count\n",
    "\n",
    "    if dataset_json_path:\n",
    "        save_path = str(save_dir / \"result.json\")\n",
    "        save_json(all_coco_preds, save_path)\n",
    "   \n",
    "    print(f\"\\nâœ… Completed {len(image_files)} images.\")\n",
    "    print(\"Total Prediction Count: \", grand_total_predictions)\n",
    "    print(\"Total Prediction time for all images is: {:.2f} ms\".format(total_prediction_time * 1000))\n",
    "    print(f\"Prediction results are successfully exported to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2d43ea5a-651d-4bd6-bcd9-2989d2945558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_preds_to_coco(preds, image_id):\n",
    "    coco_predictions = []\n",
    "    for pred in preds:\n",
    "        try:\n",
    "            coco = pred.to_coco_prediction(image_id=image_id)\n",
    "            coco_dict = {\n",
    "                \"image_id\": coco.image_id,\n",
    "                \"bbox\": coco.bbox,\n",
    "                \"score\": coco.score,\n",
    "                \"category_id\": coco.category_id,\n",
    "                \"segmentation\": coco.segmentation,\n",
    "                \"iscrowd\": coco.iscrowd,\n",
    "                \"area\": coco.area,\n",
    "            }\n",
    "            # Optionally include category_name (not used in COCO eval)\n",
    "            # coco_dict[\"category_name\"] = coco.category_name\n",
    "\n",
    "            # Only add if bbox is valid (non-empty)\n",
    "            if coco_dict[\"bbox\"]:\n",
    "                coco_predictions.append(coco_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to convert to COCO dict: {e}\")\n",
    "    return coco_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f774817a-b5f9-42bd-bbb4-720fbe889c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert merged predictions to COCO format\n",
    "def merged_preds_to_coco_bk(preds, image_id):\n",
    "    coco_predictions = []\n",
    "    for pred in preds:\n",
    "        coco_dicts = pred.to_coco_prediction(image_id=image_id)\n",
    "        print(\"__________________________________________________\")\n",
    "        print(coco_dicts)\n",
    "        for coco in coco_dicts:\n",
    "             print(coco)\n",
    "             if \"bbox\" in coco and coco[\"bbox\"]:\n",
    "                print(\"true\")\n",
    "                coco_predictions.append(coco)\n",
    "    return coco_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "39a97fd0-3228-448b-bf07-d7a09a86c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fine_sliced_images_refactored_2(input_folder, dataset_json_path, detection_model, base_slice_size=512):\n",
    "    name = \"exp\"\n",
    "    save_dir = Path(increment_path(Path(\"sliced_predictions\") / name, exist_ok=False))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if dataset_json_path:\n",
    "        with open(dataset_json_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "    vis_params = {\n",
    "        \"bbox_thickness\": 2,\n",
    "        \"text_size\": 0.5,\n",
    "        \"text_thickness\": 1,\n",
    "        \"hide_labels\": False,\n",
    "        \"hide_conf\": False,\n",
    "        \"format\": \"png\"\n",
    "    }\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nðŸš€ Running fine slicing prediction on {len(image_files)} images...\")\n",
    "    total_prediction_time = 0.0\n",
    "    grand_total_predictions = 0\n",
    "    all_coco_preds = []\n",
    "\n",
    "    for filename in image_files:\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "        image_np = np.array(image_pil)\n",
    "        image_h, image_w = image_np.shape[:2]\n",
    "        filename_wo_ext = Path(filename).stem\n",
    "        total_prediction_count = 0\n",
    "        print(\"*****************************************\")\n",
    "        print(\"File Name\", filename_wo_ext)\n",
    "\n",
    "        img_id = get_image_id(data, filename_wo_ext) if dataset_json_path else None\n",
    "        all_object_predictions = []\n",
    "\n",
    "        # Split image into 2x2 grid\n",
    "        grid_h, grid_w = image_h // 2, image_w // 2\n",
    "\n",
    "        for row in range(2):\n",
    "            for col in range(2):\n",
    "                x1, y1 = col * grid_w, row * grid_h\n",
    "                x2, y2 = min(x1 + grid_w, image_w), min(y1 + grid_h, image_h)\n",
    "                sub_img = image_pil.crop((x1, y1, x2, y2))\n",
    "                print(\"Cropped Image:\", x1, y1, x2, y2)\n",
    "\n",
    "                time_start = time.time()\n",
    "                base_pred = get_prediction(sub_img, detection_model)\n",
    "                time_end = time.time() - time_start\n",
    "                print(\"Initial Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "\n",
    "                object_density = len(base_pred.object_prediction_list)\n",
    "                print(\"Object Density:\", object_density)\n",
    "\n",
    "                slice_params = get_slice_parameters(object_density, base_slice_size)\n",
    "\n",
    "                iteration_time = time_end\n",
    "\n",
    "                if slice_params:\n",
    "                    slice_width, slice_height, overlap_w, overlap_h = slice_params\n",
    "                    print(\"********* Slice Parameters ***********\")\n",
    "                    print(\"Slice Width: \", slice_width)\n",
    "                    print(\"Slice Height: \", slice_height)\n",
    "                    print(\"Overlap Width Ratio: \", overlap_w)\n",
    "                    print(\"Overlap Height Ratio: \", overlap_h)\n",
    "\n",
    "                    time_start_slice = time.time()\n",
    "                    sliced_pred = get_sliced_prediction(\n",
    "                        sub_img,\n",
    "                        detection_model,\n",
    "                        slice_height=slice_height,\n",
    "                        slice_width=slice_width,\n",
    "                        overlap_height_ratio=overlap_h,\n",
    "                        overlap_width_ratio=overlap_w,\n",
    "                        postprocess_type=\"OptNMS\",\n",
    "                        postprocess_match_metric=\"IOU\",\n",
    "                        postprocess_match_threshold=0.3,\n",
    "                        postprocess_min_area=16,\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    time_end_slice = time.time() - time_start_slice\n",
    "                    print(\"Sliced Prediction time is: {:.2f} ms\".format(time_end_slice * 1000))\n",
    "                    \n",
    "                    iteration_time += time_end_slice\n",
    "                    preds = sliced_pred.object_prediction_list\n",
    "                else:\n",
    "                    print(\"Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "                    preds = base_pred.object_prediction_list\n",
    "\n",
    "                # Offset bounding boxes back to original image space\n",
    "                for pred in preds:\n",
    "                    pred.bbox.minx += x1\n",
    "                    pred.bbox.maxx += x1\n",
    "                    pred.bbox.miny += y1\n",
    "                    pred.bbox.maxy += y1\n",
    "                    all_object_predictions.append(pred)\n",
    "\n",
    "                total_prediction_count += len(preds)\n",
    "                total_prediction_time += iteration_time\n",
    "\n",
    "        print(\"________________________________________________\")\n",
    "\n",
    "        # Merge all predictions for image\n",
    "        merged_preds = nms_merge(all_object_predictions, iou_threshold=0.5)\n",
    "        print(\"Merge Count Prediction:\", len(merged_preds))\n",
    "\n",
    "        # Convert merged predictions to COCO format\n",
    "        if dataset_json_path:\n",
    "            coco_preds = merged_preds_to_coco(merged_preds, img_id)\n",
    "            all_coco_preds.extend(coco_preds)\n",
    "            print(\"COCO formatted Prediction Count:\", len(all_coco_preds))\n",
    "            \n",
    "        # Visualization\n",
    "        visualize_object_predictions(\n",
    "            image=np.ascontiguousarray(image_pil),\n",
    "            object_prediction_list=merged_preds,\n",
    "            rect_th=vis_params[\"bbox_thickness\"],\n",
    "            text_size=vis_params[\"text_size\"],\n",
    "            text_th=vis_params[\"text_thickness\"],\n",
    "            hide_labels=vis_params[\"hide_labels\"],\n",
    "            hide_conf=vis_params[\"hide_conf\"],\n",
    "            output_dir=save_dir,\n",
    "            file_name=filename_wo_ext,\n",
    "            export_format=vis_params[\"format\"]\n",
    "        )\n",
    "\n",
    "        grand_total_predictions += total_prediction_count\n",
    "\n",
    "    if dataset_json_path:\n",
    "        save_path = str(save_dir / \"result.json\")\n",
    "        save_json(all_coco_preds, save_path)\n",
    "   \n",
    "    print(f\"\\n Completed {len(image_files)} images.\")\n",
    "    print(\"Total Prediction Count: \", grand_total_predictions)\n",
    "    print(\"Total Prediction time for all images is: {:.2f} ms\".format(total_prediction_time * 1000))\n",
    "    print(f\"Prediction results are successfully exported to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "566afb9e-56f6-45af-8cdb-343f609ccf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running fine slicing prediction on 1 images...\n",
      "*****************************************\n",
      "File Name 0000006_06773_d_0000018\n",
      "Cropped Image: 0 0 680 382\n",
      "Initial Prediction time is: 8.59 ms\n",
      "Object Density: 15\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  256\n",
      "Slice Height:  256\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 6\n",
      "POST PROCESS:  OptNMS\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  29\n",
      "Final Bounding Box Count (OptNMS): 16\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  6\n",
      "Final Bounding Box Count (OptNMS): 5\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Sliced Prediction time is: 61.59 ms\n",
      "Cropped Image: 680 0 1360 382\n",
      "Initial Prediction time is: 7.39 ms\n",
      "Object Density: 6\n",
      "Prediction time is: 7.39 ms\n",
      "Cropped Image: 0 382 680 764\n",
      "Initial Prediction time is: 7.37 ms\n",
      "Object Density: 8\n",
      "Prediction time is: 7.37 ms\n",
      "Cropped Image: 680 382 1360 764\n",
      "Initial Prediction time is: 7.37 ms\n",
      "Object Density: 7\n",
      "Prediction time is: 7.37 ms\n",
      "________________________________________________\n",
      "IOU Threshold:  0.5\n",
      "Merge Count Prediction: 39\n",
      "COCO formatted Prediction Count: 39\n",
      "\n",
      " Completed 1 images.\n",
      "Total Prediction Count:  45\n",
      "Total Prediction time for all images is: 92.30 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#latest - OptNMS Take6\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_visdrone_test_990.json\"\n",
    "slice_size = 256\n",
    "\n",
    "# Run fine-sliced adaptive prediction\n",
    "result = predict_fine_sliced_images_refactored_2(\n",
    "    input_folder=source_folder,             # Folder with input images\n",
    "    dataset_json_path=json_path,            # Optional: COCO JSON for ID mapping (or use None)\n",
    "    detection_model=detection_model,\n",
    "    base_slice_size=slice_size              # You can try 256 or 640 depending on image/object scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8387f919-3a70-4f34-aef5-3c61d6ccdfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.422\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.754\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.493\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.976\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.750\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.435\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.479\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.291\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.483\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.520\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.329\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp320/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD (TruncatedNMS)\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './sliced_predictions/exp320/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0682cc8a-edfe-4e8d-a639-e6cb3536c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running fine slicing prediction on 1 images...\n",
      "*****************************************\n",
      "File Name 0000006_06773_d_0000018\n",
      "Cropped Image: 0 0 680 382\n",
      "Initial Prediction time is: 20.31 ms\n",
      "Object Density: 15\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  256\n",
      "Slice Height:  256\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 6\n",
      "POST PROCESS:  OptNMS\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  29\n",
      "Final Bounding Box Count (OptNMS): 16\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  6\n",
      "Final Bounding Box Count (OptNMS): 5\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Sliced Prediction time is: 61.93 ms\n",
      "Cropped Image: 680 0 1360 382\n",
      "Initial Prediction time is: 7.36 ms\n",
      "Object Density: 6\n",
      "Prediction time is: 7.36 ms\n",
      "Cropped Image: 0 382 680 764\n",
      "Initial Prediction time is: 7.32 ms\n",
      "Object Density: 8\n",
      "Prediction time is: 7.32 ms\n",
      "Cropped Image: 680 382 1360 764\n",
      "Initial Prediction time is: 7.30 ms\n",
      "Object Density: 7\n",
      "Prediction time is: 7.30 ms\n",
      "________________________________________________\n",
      "IOU Threshold:  0.3\n",
      "Merge Count Prediction: 35\n",
      "COCO formatted Prediction Count: 35\n",
      "\n",
      "âœ… Completed 1 images.\n",
      "Total Prediction Count:  45\n",
      "Total Prediction time for all images is: 104.22 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp317\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#latest - OptNMS Take6\n",
    "source_folder = './single_test/images_bk'\n",
    "json_path = \"./subset_visdrone_test_990.json\"\n",
    "slice_size = 256\n",
    "\n",
    "# Run fine-sliced adaptive prediction\n",
    "result = predict_fine_sliced_images_refactored_2(\n",
    "    input_folder=source_folder,             # Folder with input images\n",
    "    dataset_json_path=json_path,            # Optional: COCO JSON for ID mapping (or use None)\n",
    "    detection_model=detection_model,\n",
    "    base_slice_size=slice_size              # You can try 256 or 640 depending on image/object scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ea32aff5-85cc-40a0-92d4-82709a5ac343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.383\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.694\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.434\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.976\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.664\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.435\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.424\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.291\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.483\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.462\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.329\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp317/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD (TruncatedNMS)\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './sliced_predictions/exp317/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "66667a05-9505-4241-bd87-924c5b2bc426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST PROCESSING: NMS\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 361.86it/s]\n",
      "Performing inference on images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: 0000006_06773_d_0000018\n",
      "Image Size:  (1360, 765)\n",
      "Sliced Boxes Count: 50\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 50 slices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction Count 143\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  29\n",
      "Final Bounding Box Count (NMS):  2\n",
      "Final Bounding Box Count (NMS):  12\n",
      "Final Bounding Box Count (NMS):  6\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Prediction time is: 491.99 ms\n",
      "Prediction results are successfully exported to runs/predict/exp235\n",
      "Model loaded in 0.032637596130371094 seconds.\n",
      "Slicing performed in 0.0013730525970458984 seconds.\n",
      "Prediction performed in 0.4919910430908203 seconds.\n",
      "Exporting performed in 0.049646854400634766 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Threshold IOU = 0.5\n",
    "result_predict_nms = predict(source='./single_test',\n",
    "                         dataset_json_path = './subset_visdrone_test_990.json',\n",
    "                         model_type = 'ultralytics',\n",
    "                         model_path = 'models/yolov8/last.pt',\n",
    "                         slice_height = 256,\n",
    "                         slice_width = 256,\n",
    "                         overlap_height_ratio = 0.5,\n",
    "                         overlap_width_ratio = 0.5,\n",
    "                         postprocess_type = \"NMS\",\n",
    "                         postprocess_min_area = 16,\n",
    "                         postprocess_conf_threshold = 0.5,                  \n",
    "                         verbose = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0e645f14-69a4-492b-b60c-698053bf3d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.391\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.704\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.426\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.958\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.741\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.287\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.402\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.468\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.159\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.515\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.157\n",
      "COCO evaluation results are successfully exported to runs/predict/exp235/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD (NMS)\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './runs/predict/exp235/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6f8562f8-eec9-4934-b2cc-662437d60f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST PROCESSING: NMS\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 365.93it/s]\n",
      "Performing inference on images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: 0000006_06773_d_0000018\n",
      "Image Size:  (1360, 765)\n",
      "Sliced Boxes Count: 50\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 50 slices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction Count 143\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  29\n",
      "Final Bounding Box Count (NMS):  2\n",
      "Final Bounding Box Count (NMS):  12\n",
      "Final Bounding Box Count (NMS):  6\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Prediction time is: 488.65 ms\n",
      "Prediction results are successfully exported to runs/predict/exp234\n",
      "Model loaded in 0.10470438003540039 seconds.\n",
      "Slicing performed in 0.0013213157653808594 seconds.\n",
      "Prediction performed in 0.4886488914489746 seconds.\n",
      "Exporting performed in 0.0502934455871582 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_predict_nms = predict(source='./single_test',\n",
    "                         dataset_json_path = './subset_visdrone_test_990.json',\n",
    "                         model_type = 'ultralytics',\n",
    "                         model_path = 'models/yolov8/last.pt',\n",
    "                         slice_height = 256,\n",
    "                         slice_width = 256,\n",
    "                         overlap_height_ratio = 0.5,\n",
    "                         overlap_width_ratio = 0.5,\n",
    "                         postprocess_type = \"NMS\",\n",
    "                         postprocess_min_area = 16,\n",
    "                         postprocess_conf_threshold = 0.3,                  \n",
    "                         verbose = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bb969ff8-a9c6-4be9-9024-d23487903362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.391\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.704\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.426\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.958\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.741\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.287\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.402\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.468\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.159\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.515\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.157\n",
      "COCO evaluation results are successfully exported to runs/predict/exp234/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD (NMS)\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_990.json' --result_json_path './runs/predict/exp234/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "691bc31e-e1d7-4c4f-813a-6fa81920e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running fine slicing prediction on 1 images...\n",
      "*****************************************\n",
      "File Name 0000011_05068_d_0000008\n",
      "Cropped Image: 0 0 680 382\n",
      "Initial Prediction time is: 9.78 ms\n",
      "Object Density: 13\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  256\n",
      "Slice Height:  256\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 6\n",
      "POST PROCESS:  OptNMS\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  24\n",
      "Final Bounding Box Count (OptNMS): 18\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Sliced Prediction time is: 61.60 ms\n",
      "Cropped Image: 680 0 1360 382\n",
      "Initial Prediction time is: 7.46 ms\n",
      "Object Density: 20\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  256\n",
      "Slice Height:  256\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 6\n",
      "POST PROCESS:  OptNMS\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  25\n",
      "Final Bounding Box Count (OptNMS): 21\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  3\n",
      "Final Bounding Box Count (OptNMS): 3\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  7\n",
      "Final Bounding Box Count (OptNMS): 7\n",
      "Sliced Prediction time is: 60.97 ms\n",
      "Cropped Image: 0 382 680 764\n",
      "Initial Prediction time is: 7.28 ms\n",
      "Object Density: 2\n",
      "Prediction time is: 7.28 ms\n",
      "Cropped Image: 680 382 1360 764\n",
      "Initial Prediction time is: 7.28 ms\n",
      "Object Density: 7\n",
      "Prediction time is: 7.28 ms\n",
      "________________________________________________\n",
      "IOU Threshold:  0.3\n",
      "Merge Count Prediction: 51\n",
      "COCO formatted Prediction Count: 51\n",
      "\n",
      "âœ… Completed 1 images.\n",
      "Total Prediction Count:  71\n",
      "Total Prediction time for all images is: 154.37 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp316\n"
     ]
    }
   ],
   "source": [
    "#latest - OptNMS Take6\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1162.json\"\n",
    "slice_size = 256\n",
    "\n",
    "# Run fine-sliced adaptive prediction\n",
    "result = predict_fine_sliced_images_refactored_2(\n",
    "    input_folder=source_folder,             # Folder with input images\n",
    "    dataset_json_path=json_path,            # Optional: COCO JSON for ID mapping (or use None)\n",
    "    detection_model=detection_model,\n",
    "    base_slice_size=slice_size              # You can try 256 or 640 depending on image/object scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a5efe90b-624e-4e80-9bc2-f50371e3f4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.120\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.129\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.211\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.044\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.119\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.095\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.125\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = -1.000\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp316/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD (TruncatedNMS)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1162.json' --result_json_path './sliced_predictions/exp316/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "21e2f16f-d696-4081-b8b1-6ec1d2fa5ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.120\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.129\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.211\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.044\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.119\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.095\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.125\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = -1.000\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp315/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD (TruncatedNMS)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1162.json' --result_json_path './sliced_predictions/exp315/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91eecacf-ccc0-4ff1-a8aa-55b641eecdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.019\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.043\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.025\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = -1.000\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp272/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD (OptNMS)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1162.json' --result_json_path './sliced_predictions/exp272/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0a117065-178f-4041-840e-7040e65c739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running fine slicing prediction on 1 images...\n",
      "*****************************************\n",
      "File Name 0000011_05068_d_0000008\n",
      "Cropped Image: 0 0 680 382\n",
      "Initial Prediction time is: 8.67 ms\n",
      "Object Density: 13\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  256\n",
      "Slice Height:  256\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 6\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 32\n",
      "Intial Min Area:  16\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  24\n",
      "Final Bounding Box Count (OptNMS): 18\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Sliced Prediction time is: 60.38 ms\n",
      "Cropped Image: 680 0 1360 382\n",
      "Initial Prediction time is: 7.49 ms\n",
      "Object Density: 20\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  256\n",
      "Slice Height:  256\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 6\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 41\n",
      "Intial Min Area:  16\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  25\n",
      "Final Bounding Box Count (OptNMS): 21\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  3\n",
      "Final Bounding Box Count (OptNMS): 3\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  7\n",
      "Final Bounding Box Count (OptNMS): 7\n",
      "Sliced Prediction time is: 60.30 ms\n",
      "Cropped Image: 0 382 680 764\n",
      "Initial Prediction time is: 7.29 ms\n",
      "Object Density: 2\n",
      "Prediction time is: 7.29 ms\n",
      "Cropped Image: 680 382 1360 764\n",
      "Initial Prediction time is: 7.21 ms\n",
      "Object Density: 7\n",
      "Prediction time is: 7.21 ms\n",
      "________________________________________________\n",
      "Merge Count Prediction: 51\n",
      "\n",
      "âœ… Completed 1 images.\n",
      "Total Prediction Count:  71\n",
      "Total Prediction time for all images is: 151.34 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp270\n"
     ]
    }
   ],
   "source": [
    "#latest - OptNMS Take5\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1162.json\"\n",
    "slice_size = 256\n",
    "\n",
    "# Run fine-sliced adaptive prediction\n",
    "predict_fine_sliced_images_refactored(\n",
    "    input_folder=source_folder,             # Folder with input images\n",
    "    dataset_json_path=json_path,            # Optional: COCO JSON for ID mapping (or use None)\n",
    "    detection_model=detection_model,\n",
    "    base_slice_size=slice_size              # You can try 256 or 640 depending on image/object scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3d9b2f3d-a734-49d3-bc38-453ca48ba0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.019\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.043\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.025\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = -1.000\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp270/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD (OptNMS)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1162.json' --result_json_path './sliced_predictions/exp270/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d4494f7-0ef4-4d4b-8152-fa6345af41fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST PROCESSING: NMS\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 444.08it/s]\n",
      "Performing inference on images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: 0000011_05068_d_0000008\n",
      "Image Size:  (1360, 765)\n",
      "Sliced Boxes Count: 50\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 50 slices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction Count 271\n",
      "Final Bounding Box Count (NMS):  44\n",
      "Final Bounding Box Count (NMS):  7\n",
      "Final Bounding Box Count (NMS):  30\n",
      "Final Bounding Box Count (NMS):  22\n",
      "Final Bounding Box Count (NMS):  5\n",
      "Final Bounding Box Count (NMS):  2\n",
      "Final Bounding Box Count (NMS):  2\n",
      "Final Bounding Box Count (NMS):  11\n",
      "Prediction time is: 510.31 ms\n",
      "Prediction results are successfully exported to runs/predict/exp233\n",
      "Model loaded in 0.18946099281311035 seconds.\n",
      "Slicing performed in 0.001336812973022461 seconds.\n",
      "Prediction performed in 0.5103139877319336 seconds.\n",
      "Exporting performed in 0.04958391189575195 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_predict_nms = predict(source='./single_test',\n",
    "                         dataset_json_path = './subset_vis_test_data_1162.json',\n",
    "                         model_type = 'ultralytics',\n",
    "                         model_path = 'models/yolov8/last.pt',\n",
    "                         slice_height = 256,\n",
    "                         slice_width = 256,\n",
    "                         overlap_height_ratio = 0.5,\n",
    "                         overlap_width_ratio = 0.5,\n",
    "                         postprocess_type = \"NMS\",\n",
    "                         postprocess_min_area = 16,\n",
    "                         postprocess_conf_threshold = 0.3,                  \n",
    "                         verbose = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3a06245-ed8e-4fec-b6db-1174e1f25129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.080\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.189\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.038\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.169\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.429\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.252\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.081\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.312\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = -1.000\n",
      "COCO evaluation results are successfully exported to runs/predict/exp233/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive-Optimized-NMS-IoU METHOD (OptNMS)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1162.json' --result_json_path './runs/predict/exp233/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0ce8ba8-740e-4e35-9a46-2257d8a7dde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running fine slicing prediction on 1 images...\n",
      "*****************************************\n",
      "File Name 0000011_05068_d_0000008\n",
      "Cropped Image: 0 0 680 382\n",
      "Initial Prediction time is: 8.60 ms\n",
      "Object Density: 13\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  256\n",
      "Slice Height:  256\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 6\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 32\n",
      "Intial Min Area:  16\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  24\n",
      "Final Bounding Box Count (OptNMS): 18\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Sliced Prediction time is: 61.05 ms\n",
      "Sliced Level Prediction Count 1:  25\n",
      "Cropped Image: 680 0 1360 382\n",
      "Initial Prediction time is: 7.41 ms\n",
      "Object Density: 20\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  256\n",
      "Slice Height:  256\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 6\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 41\n",
      "Intial Min Area:  16\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  25\n",
      "Final Bounding Box Count (OptNMS): 21\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  3\n",
      "Final Bounding Box Count (OptNMS): 3\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  7\n",
      "Final Bounding Box Count (OptNMS): 7\n",
      "Sliced Prediction time is: 60.15 ms\n",
      "Sliced Level Prediction Count 1:  37\n",
      "Cropped Image: 0 382 680 764\n",
      "Initial Prediction time is: 7.29 ms\n",
      "Object Density: 2\n",
      "Prediction time is: 7.29 ms\n",
      "Sliced Level Prediction Count 2:  2\n",
      "Cropped Image: 680 382 1360 764\n",
      "Initial Prediction time is: 7.29 ms\n",
      "Object Density: 7\n",
      "Prediction time is: 7.29 ms\n",
      "Sliced Level Prediction Count 2:  7\n",
      "\n",
      "âœ… Completed 1 images.\n",
      "Total Prediction Count:  71\n",
      "Total Prediction time for all images is: 151.79 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp240\n"
     ]
    }
   ],
   "source": [
    "#latest - OptNMS Take4\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1162.json\"\n",
    "slice_size = 256\n",
    "\n",
    "# Run fine-sliced adaptive prediction\n",
    "predict_fine_sliced_images(\n",
    "    input_folder=source_folder,             # Folder with input images\n",
    "    dataset_json_path=json_path,            # Optional: COCO JSON for ID mapping (or use None)\n",
    "    detection_model=detection_model,\n",
    "    base_slice_size=slice_size              # You can try 256 or 640 depending on image/object scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6c11366-caf6-4cac-b6bf-e986fee88785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running fine slicing prediction on 1 images...\n",
      "*****************************************\n",
      "File Name 0000011_05068_d_0000008\n",
      "Cropped Image: 0 0 680 382\n",
      "Object Density: 13\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  256\n",
      "Slice Height:  256\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 6\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 32\n",
      "Intial Min Area:  16\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  24\n",
      "Final Bounding Box Count (OptNMS): 18\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Sliced Level Prediction Count 1:  25\n",
      "Cropped Image: 680 0 1360 382\n",
      "Object Density: 20\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  256\n",
      "Slice Height:  256\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 6\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 41\n",
      "Intial Min Area:  16\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  25\n",
      "Final Bounding Box Count (OptNMS): 21\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  3\n",
      "Final Bounding Box Count (OptNMS): 3\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  7\n",
      "Final Bounding Box Count (OptNMS): 7\n",
      "Sliced Level Prediction Count 1:  37\n",
      "Cropped Image: 0 382 680 764\n",
      "Object Density: 2\n",
      "Sliced Level Prediction Count 2:  2\n",
      "Cropped Image: 680 382 1360 764\n",
      "Object Density: 7\n",
      "Sliced Level Prediction Count 2:  7\n",
      "\n",
      "âœ… Completed 1 images.\n",
      "Total Prediction Count:  71\n",
      "Prediction results are successfully exported to sliced_predictions/exp238\n"
     ]
    }
   ],
   "source": [
    "#latest - OptNMS Take3\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1162.json\"\n",
    "slice_size = 256\n",
    "\n",
    "# Run fine-sliced adaptive prediction\n",
    "predict_fine_sliced_images(\n",
    "    input_folder=source_folder,             # Folder with input images\n",
    "    dataset_json_path=json_path,  # Optional: COCO JSON for ID mapping (or use None)\n",
    "    detection_model=detection_model,\n",
    "    base_slice_size=slice_size                        # You can try 256 or 640 depending on image/object scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c10ad2b8-d51d-496f-964f-dd15dae35efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running fine slicing prediction on 1 images...\n",
      "*****************************************\n",
      "File Name 0000011_05068_d_0000008\n",
      "Cropped Image: 0 0 680 382\n",
      "Object Density: 13\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 2\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 37\n",
      "Intial Min Area:  16\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  32\n",
      "Final Bounding Box Count (OptNMS): 20\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Sliced Level Prediction Count 1:  24\n",
      "Cropped Image: 680 0 1360 382\n",
      "Object Density: 20\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 2\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 41\n",
      "Intial Min Area:  16\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  23\n",
      "Final Bounding Box Count (OptNMS): 15\n",
      "Adaptive Filtered Prediction:  5\n",
      "Final Bounding Box Count (OptNMS): 5\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  10\n",
      "Final Bounding Box Count (OptNMS): 9\n",
      "Sliced Level Prediction Count 1:  32\n",
      "Cropped Image: 0 382 680 764\n",
      "Object Density: 2\n",
      "Sliced Level Prediction Count 2:  2\n",
      "Cropped Image: 680 382 1360 764\n",
      "Object Density: 7\n",
      "Sliced Level Prediction Count 2:  7\n",
      "\n",
      "âœ… Completed 1 images.\n",
      "Total Prediction Count:  65\n",
      "Prediction results are successfully exported to sliced_predictions/exp237\n"
     ]
    }
   ],
   "source": [
    "#latest - OptNMS Take2\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1162.json\"\n",
    "slice_size = 512\n",
    "\n",
    "# Run fine-sliced adaptive prediction\n",
    "predict_fine_sliced_images(\n",
    "    input_folder=source_folder,             # Folder with input images\n",
    "    dataset_json_path=json_path,  # Optional: COCO JSON for ID mapping (or use None)\n",
    "    detection_model=detection_model,\n",
    "    base_slice_size=slice_size                        # You can try 256 or 640 depending on image/object scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aae8283-532f-4ab6-862d-972dc34243b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running fine slicing prediction on 1 images...\n",
      "*****************************************\n",
      "File Name 0000011_05068_d_0000008\n",
      "Cropped Image: 0 0 680 382\n",
      "Object Density: 13\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 2\n",
      "POST PROCESS:  NMS\n",
      "Original Prediction Count 37\n",
      "Final Bounding Box Count (NMS):  15\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Sliced Level Prediction Count:  19\n",
      "Total Prediction Count:  19\n",
      "Cropped Image: 680 0 1360 382\n",
      "Object Density: 20\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 2\n",
      "POST PROCESS:  NMS\n",
      "Original Prediction Count 41\n",
      "Final Bounding Box Count (NMS):  10\n",
      "Final Bounding Box Count (NMS):  4\n",
      "Final Bounding Box Count (NMS):  2\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  5\n",
      "Sliced Level Prediction Count:  22\n",
      "Total Prediction Count:  41\n",
      "Cropped Image: 0 382 680 764\n",
      "Object Density: 2\n",
      "Sliced Level Prediction Count:  2\n",
      "Total Prediction Count:  43\n",
      "Cropped Image: 680 382 1360 764\n",
      "Object Density: 7\n",
      "Sliced Level Prediction Count:  7\n",
      "Total Prediction Count:  50\n",
      "\n",
      "âœ… Completed 1 images.\n",
      "Prediction results are successfully exported to sliced_predictions/exp235\n"
     ]
    }
   ],
   "source": [
    "#latest - NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1162.json\"\n",
    "slice_size = 512\n",
    "\n",
    "# Run fine-sliced adaptive prediction\n",
    "predict_fine_sliced_images(\n",
    "    input_folder=source_folder,             # Folder with input images\n",
    "    dataset_json_path=json_path,  # Optional: COCO JSON for ID mapping (or use None)\n",
    "    detection_model=detection_model,\n",
    "    base_slice_size=slice_size                        # You can try 256 or 640 depending on image/object scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "535e0f4a-ba55-4489-839d-1fcaecdbe06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running fine slicing prediction on 1 images...\n",
      "*****************************************\n",
      "File Name 0000011_05068_d_0000008\n",
      "Cropped Image: 0 0 680 382\n",
      "Object Density: 13\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 2\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 37\n",
      "Intial Min Area:  16\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  32\n",
      "Final Bounding Box Count (OptNMS): 20\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Sliced Level Prediction Count:  24\n",
      "Total Prediction Count:  24\n",
      "Cropped Image: 680 0 1360 382\n",
      "Object Density: 20\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 2\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 41\n",
      "Intial Min Area:  16\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  16\n",
      "Adaptive Filtered Prediction:  23\n",
      "Final Bounding Box Count (OptNMS): 15\n",
      "Adaptive Filtered Prediction:  5\n",
      "Final Bounding Box Count (OptNMS): 5\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  10\n",
      "Final Bounding Box Count (OptNMS): 9\n",
      "Sliced Level Prediction Count:  32\n",
      "Total Prediction Count:  56\n",
      "Cropped Image: 0 382 680 764\n",
      "Object Density: 2\n",
      "Sliced Level Prediction Count:  2\n",
      "Total Prediction Count:  58\n",
      "Cropped Image: 680 382 1360 764\n",
      "Object Density: 7\n",
      "Sliced Level Prediction Count:  7\n",
      "Total Prediction Count:  65\n",
      "\n",
      "âœ… Completed 1 images.\n",
      "Prediction results are successfully exported to sliced_predictions/exp234\n"
     ]
    }
   ],
   "source": [
    "#latest - OptNMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1162.json\"\n",
    "slice_size = 512\n",
    "\n",
    "# Run fine-sliced adaptive prediction\n",
    "predict_fine_sliced_images(\n",
    "    input_folder=source_folder,             # Folder with input images\n",
    "    dataset_json_path=json_path,  # Optional: COCO JSON for ID mapping (or use None)\n",
    "    detection_model=detection_model,\n",
    "    base_slice_size=slice_size                        # You can try 256 or 640 depending on image/object scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60531d1c-9170-486e-8d20-17b8b39bbec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running fine slicing prediction on 1 images...\n",
      "*****************************************\n",
      "File Name 0000011_05068_d_0000008\n",
      "Cropped Image: 0 0 680 382\n",
      "Object Density: 13\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 2\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 37\n",
      "Intial Min Area:  32\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  32\n",
      "Adaptive Filtered Prediction:  32\n",
      "Final Bounding Box Count (OptNMS): 20\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Sliced Level Prediction Count:  24\n",
      "Total Prediction Count:  24\n",
      "Cropped Image: 680 0 1360 382\n",
      "Object Density: 20\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "Sliced Boxes Count: 2\n",
      "POST PROCESS:  OptNMS\n",
      "Original Prediction Count 41\n",
      "Intial Min Area:  32\n",
      "Confidence Scores:  0.3\n",
      "Min area Threshold:  32\n",
      "Adaptive Filtered Prediction:  23\n",
      "Final Bounding Box Count (OptNMS): 15\n",
      "Adaptive Filtered Prediction:  5\n",
      "Final Bounding Box Count (OptNMS): 5\n",
      "Adaptive Filtered Prediction:  2\n",
      "Final Bounding Box Count (OptNMS): 2\n",
      "Adaptive Filtered Prediction:  1\n",
      "Final Bounding Box Count (OptNMS): 1\n",
      "Adaptive Filtered Prediction:  10\n",
      "Final Bounding Box Count (OptNMS): 9\n",
      "Sliced Level Prediction Count:  32\n",
      "Total Prediction Count:  56\n",
      "Cropped Image: 0 382 680 764\n",
      "Object Density: 2\n",
      "Sliced Level Prediction Count:  2\n",
      "Total Prediction Count:  58\n",
      "Cropped Image: 680 382 1360 764\n",
      "Object Density: 7\n",
      "Sliced Level Prediction Count:  7\n",
      "Total Prediction Count:  65\n",
      "\n",
      "âœ… Completed 1 images.\n",
      "Prediction results are successfully exported to sliced_predictions/exp225\n"
     ]
    }
   ],
   "source": [
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1162.json\"\n",
    "slice_size = 512\n",
    "\n",
    "# Run fine-sliced adaptive prediction\n",
    "predict_fine_sliced_images(\n",
    "    input_folder=source_folder,             # Folder with input images\n",
    "    dataset_json_path=json_path,  # Optional: COCO JSON for ID mapping (or use None)\n",
    "    detection_model=detection_model,\n",
    "    base_slice_size=slice_size                        # You can try 256 or 640 depending on image/object scale\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526cb18-5df7-4314-879f-41609efa5455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022cd21-44cf-4455-99b2-01618bd86e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_fine_slicing(args):\n",
    "    filename, input_folder, detection_model, base_slice_size, data, save_dir, vis_params = args\n",
    "    image_path = os.path.join(input_folder, filename)\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "    image_np = np.array(image_pil)\n",
    "    image_h, image_w = image_np.shape[:2]\n",
    "    filename_wo_ext = Path(filename).stem\n",
    "    img_id = next((img[\"id\"] for img in data.get(\"images\", []) if img[\"file_name\"].startswith(filename_wo_ext)), None)\n",
    "\n",
    "    print(\"*****************************************\")\n",
    "    print(\"File Name\", filename_wo_ext)\n",
    "    \n",
    "    all_object_predictions = []\n",
    "    total_time = 0\n",
    "\n",
    "    # Split image into 2x2 grid\n",
    "    grid_h, grid_w = image_h // 2, image_w // 2\n",
    "\n",
    "    for row in range(2):\n",
    "        for col in range(2):\n",
    "            x1, y1 = col * grid_w, row * grid_h\n",
    "            x2, y2 = min(x1 + grid_w, image_w), min(y1 + grid_h, image_h)\n",
    "            sub_img = image_pil.crop((x1, y1, x2, y2)) #grid cell\n",
    "\n",
    "            # Base prediction on the sub-image\n",
    "            base_pred = get_prediction(sub_img, detection_model)\n",
    "            object_density = len(base_pred.object_prediction_list)\n",
    "\n",
    "            print(\"Object Density:\", object_density)\n",
    "            \n",
    "            slice_params = get_slice_parameters(object_density, base_slice_size)\n",
    "\n",
    "            if slice_params:\n",
    "                slice_width, slice_height, overlap_w, overlap_h = slice_params\n",
    "\n",
    "                print(\"********* Slice Parameters ***********\")\n",
    "                print(\"Slice Width: \", slice_width)\n",
    "                print(\"Slice Height: \", slice_height)\n",
    "                print(\"Overlap Width Ratio: \", overlap_w)\n",
    "                print(\"Overlap Height Ratio: \", overlap_h)\n",
    "                \n",
    "                sliced_pred = get_sliced_prediction(\n",
    "                    sub_img,\n",
    "                    detection_model,\n",
    "                    slice_height=slice_height,\n",
    "                    slice_width=slice_width,\n",
    "                    overlap_height_ratio=overlap_h,\n",
    "                    overlap_width_ratio=overlap_w,\n",
    "                    postprocess_type=\"OptNMS\",\n",
    "                    postprocess_match_metric=\"IOU\",\n",
    "                    postprocess_match_threshold=0.3,\n",
    "                    postprocess_min_area=32,\n",
    "                    verbose=0\n",
    "                )\n",
    "                preds = sliced_pred.object_prediction_list\n",
    "            else:\n",
    "                preds = base_pred.object_prediction_list\n",
    "\n",
    "            # Offset predictions to original image coordinates\n",
    "            for pred in preds:\n",
    "                pred.shift(x_offset=x1, y_offset=y1)\n",
    "                all_object_predictions.append(pred)\n",
    "\n",
    "    # Apply NMS to merged predictions\n",
    "    merged_preds = nms_merge(all_object_predictions, iou_threshold=0.3)\n",
    "\n",
    "    # Visualization\n",
    "    visualize_object_predictions(\n",
    "        image=np.ascontiguousarray(image_pil),\n",
    "        object_prediction_list=merged_preds,\n",
    "        rect_th=vis_params[\"bbox_thickness\"],\n",
    "        text_size=vis_params[\"text_size\"],\n",
    "        text_th=vis_params[\"text_thickness\"],\n",
    "        hide_labels=vis_params[\"hide_labels\"],\n",
    "        hide_conf=vis_params[\"hide_conf\"],\n",
    "        output_dir=save_dir,\n",
    "        file_name=filename_wo_ext,\n",
    "        export_format=vis_params[\"format\"]\n",
    "    )\n",
    "\n",
    "    # COCO conversion\n",
    "    coco_preds = [p.to_coco_dict(image_id=img_id) for p in merged_preds]\n",
    "    return coco_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69b5cc-eeef-440d-802a-de248d7528ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sahi.prediction import get_prediction, get_sliced_prediction\n",
    "from sahi.utils.file import save_json, increment_path\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.merging import merge_object_prediction_list\n",
    "from sahi.visualization import visualize_object_predictions\n",
    "\n",
    "\n",
    "def get_dynamic_slice_parameters(object_density, base_slice_size):\n",
    "    if object_density > 30:\n",
    "        return base_slice_size // 2, base_slice_size // 2, 0.4, 0.4\n",
    "    elif object_density > 15:\n",
    "        return base_slice_size // 2, base_slice_size // 2, 0.3, 0.3\n",
    "    elif object_density > 5:\n",
    "        return base_slice_size, base_slice_size, 0.2, 0.2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_image_fine_slicing(args):\n",
    "    filename, input_folder, detection_model, base_slice_size, data, save_dir, vis_params = args\n",
    "    image_path = os.path.join(input_folder, filename)\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "    image_np = np.array(image_pil)\n",
    "    image_h, image_w = image_np.shape[:2]\n",
    "    filename_wo_ext = Path(filename).stem\n",
    "    img_id = next((img[\"id\"] for img in data.get(\"images\", []) if img[\"file_name\"].startswith(filename_wo_ext)), None)\n",
    "\n",
    "    all_object_predictions = []\n",
    "    total_time = 0\n",
    "\n",
    "    # Split image into 2x2 grid\n",
    "    grid_h, grid_w = image_h // 2, image_w // 2\n",
    "\n",
    "    for row in range(2):\n",
    "        for col in range(2):\n",
    "            x1, y1 = col * grid_w, row * grid_h\n",
    "            x2, y2 = min(x1 + grid_w, image_w), min(y1 + grid_h, image_h)\n",
    "            sub_img = image_pil.crop((x1, y1, x2, y2))\n",
    "\n",
    "            # Base prediction on the sub-image\n",
    "            base_pred = get_prediction(sub_img, detection_model)\n",
    "            object_density = len(base_pred.object_prediction_list)\n",
    "\n",
    "            slice_params = get_dynamic_slice_parameters(object_density, base_slice_size)\n",
    "\n",
    "            if slice_params:\n",
    "                slice_width, slice_height, overlap_w, overlap_h = slice_params\n",
    "                sliced_pred = get_sliced_prediction(\n",
    "                    sub_img,\n",
    "                    detection_model,\n",
    "                    slice_height=slice_height,\n",
    "                    slice_width=slice_width,\n",
    "                    overlap_height_ratio=overlap_h,\n",
    "                    overlap_width_ratio=overlap_w,\n",
    "                    postprocess_type=\"NMS\",\n",
    "                    postprocess_match_metric=\"IOU\",\n",
    "                    postprocess_match_threshold=0.5,\n",
    "                    postprocess_min_area=32,\n",
    "                    verbose=0\n",
    "                )\n",
    "                preds = sliced_pred.object_prediction_list\n",
    "            else:\n",
    "                preds = base_pred.object_prediction_list\n",
    "\n",
    "            # Offset predictions to original image coordinates\n",
    "            for pred in preds:\n",
    "                pred.shift(x_offset=x1, y_offset=y1)\n",
    "                all_object_predictions.append(pred)\n",
    "\n",
    "    # Merge predictions\n",
    "    merged_preds = merge_object_prediction_list(all_object_predictions)\n",
    "\n",
    "    # Visualization\n",
    "    visualize_object_predictions(\n",
    "        image=np.ascontiguousarray(image_pil),\n",
    "        object_prediction_list=merged_preds,\n",
    "        rect_th=vis_params[\"bbox_thickness\"],\n",
    "        text_size=vis_params[\"text_size\"],\n",
    "        text_th=vis_params[\"text_thickness\"],\n",
    "        hide_labels=vis_params[\"hide_labels\"],\n",
    "        hide_conf=vis_params[\"hide_conf\"],\n",
    "        output_dir=save_dir,\n",
    "        file_name=filename_wo_ext,\n",
    "        export_format=vis_params[\"format\"]\n",
    "    )\n",
    "\n",
    "    # COCO conversion\n",
    "    coco_preds = [p.to_coco_dict(image_id=img_id) for p in merged_preds]\n",
    "    return coco_preds\n",
    "\n",
    "\n",
    "def predict_fine_sliced_images(input_folder, dataset_json_path, detection_model, base_slice_size=512):\n",
    "    name = \"exp\"\n",
    "    save_dir = Path(increment_path(Path(\"sliced_predictions\") / name, exist_ok=False))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    data = {}\n",
    "    if dataset_json_path:\n",
    "        with open(dataset_json_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "    vis_params = {\n",
    "        \"bbox_thickness\": 2,\n",
    "        \"text_size\": 0.5,\n",
    "        \"text_thickness\": 1,\n",
    "        \"hide_labels\": False,\n",
    "        \"hide_conf\": False,\n",
    "        \"format\": \"png\"\n",
    "    }\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    tasks = [\n",
    "        (filename, input_folder, detection_model, base_slice_size, data, save_dir, vis_params)\n",
    "        for filename in image_files\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nðŸš€ Running fine slicing prediction on {len(image_files)} images...\")\n",
    "\n",
    "    all_coco_preds = []\n",
    "    with Pool(processes=min(cpu_count(), 4)) as pool:\n",
    "        for preds in pool.imap_unordered(process_image_fine_slicing, tasks):\n",
    "            all_coco_preds.extend(preds)\n",
    "\n",
    "    if dataset_json_path:\n",
    "        save_json(all_coco_preds, str(save_dir / \"result.json\"))\n",
    "        print(f\"\\nðŸ“¦ Saved COCO results to {save_dir / 'result.json'}\")\n",
    "\n",
    "    print(f\"\\nâœ… Completed {len(image_files)} images.\")\n",
    "    return all_coco_preds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch11.8]",
   "language": "python",
   "name": "conda-env-torch11.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
