{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faeab81f-e707-49df-950b-49ee30bce380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required functions and classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image, read_image_as_pil\n",
    "from sahi.utils.file import Path, increment_path, list_files, save_json, save_pickle, download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict, agg_prediction, get_prediction_batched, get_sliced_prediction_batched \n",
    "from sahi.prediction import visualize_object_predictions\n",
    "from IPython.display import Image\n",
    "from numpy import asarray\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36fe85d7-d5fa-4868-b693-263fcc9d6563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download YOLOv8-S model to 'models/yolov8s.pt'\n",
    "yolov11_model_path = 'models/yolo11/last.pt'\n",
    "yolov8_model_path = 'models/yolov8/last.pt'\n",
    "#download_yolov8s_model(destination_path=yolov8_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0da474-6a20-4128-a1c3-1dee53bee3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='ultralytics',\n",
    "    model_path=yolov8_model_path,\n",
    "    confidence_threshold=0.3,\n",
    "    device=\"cuda:0\", # or 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1cb04e-7946-4fd5-8091-606af6aedd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='ultralytics',\n",
    "    model_path=yolov8_model_path,\n",
    "    confidence_threshold=0.3,\n",
    "    device=\"cuda:0\", # or 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a68f02e8-9cfe-421f-aeca-5457125d386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def get_slice_parameters(object_density):\n",
    "    \n",
    "    #image_path = \"test_data/0000006_06773_d_0000018.jpg\"\n",
    "    #image = Image.open(image_path).convert(\"RGB\")\n",
    "    #image_width, image_height  = image.size\n",
    "    #slice_width = image_width\n",
    "    #slice_height = image_height\n",
    "    #overlap_width_ratio = 0.0\n",
    "    #overlap_height_ratio = 0.0\n",
    "\n",
    "    if object_density >= 50:\n",
    "        slice_width = 256\n",
    "        slice_height = 256\n",
    "        overlap_width_ratio = 0.5\n",
    "        overlap_height_ratio = 0.5\n",
    "    elif 25 <= object_density < 50:\n",
    "        slice_width = 256\n",
    "        slice_height = 256\n",
    "        overlap_width_ratio = 0.25\n",
    "        overlap_height_ratio = 0.25\n",
    "    elif 10 <= object_density < 25:\n",
    "        slice_width = 256\n",
    "        slice_height = 256\n",
    "        overlap_width_ratio = 0.15\n",
    "        overlap_height_ratio = 0.15\n",
    "    else:\n",
    "        slice_width = 1024\n",
    "        slice_height = 1024\n",
    "        overlap_width_ratio = 0.15\n",
    "        overlap_height_ratio = 0.15\n",
    "\n",
    "    return slice_width, slice_height, overlap_width_ratio, overlap_height_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b01fcdd4-edc8-4af9-ad95-c9e75f76a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get image details by image_id\n",
    "def get_image_id(coco_data, image_name):\n",
    "    for image in coco_data[\"images\"]:\n",
    "        file_name = Path(image['file_name']).stem\n",
    "        if file_name == image_name:\n",
    "            return image['id']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106d68fc-9890-4b63-ad10-186d29c1ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get image details by image_id\n",
    "def get_image_info(coco_data, image_name):\n",
    "    for image in coco_data[\"images\"]:\n",
    "        file_name = Path(image['file_name']).stem\n",
    "        if file_name == image_name:\n",
    "            return image\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3041a555-75f7-46ca-89c6-99e13f07c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataset_json_path = \"./subset_visdrone_test_data.json\"\n",
    "with open(dataset_json_path, \"r\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9634a91a-6d92-4d3e-b4fb-bdf363637494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000006_05208_d_0000014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(765,\n",
       " 324,\n",
       " {'height': 765,\n",
       "  'width': 1360,\n",
       "  'id': 324,\n",
       "  'file_name': 'images/0000006_05208_d_0000014.jpg'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = 'test_data/0000006_05208_d_0000014.jpg'\n",
    "#image_as_pil = read_image_as_pil(image_path)\n",
    "filename_without_ext = '0000006_05208_d_0000014'#Path(filename).stem\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"File Name\", filename_without_ext)\n",
    "\n",
    "img = get_image_info(data, filename_without_ext)\n",
    "\n",
    "img['height'], img['id'], img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b23c01f4-5f22-41c3-b60c-a32d53844168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sahi.prediction import ObjectPrediction, PredictionResult\n",
    "\n",
    "# export visualization\n",
    "def predict_sliced_images_modified(input_folder, dataset_json_path, detection_model, slice_detection_model):\n",
    "    \"\"\"\n",
    "    Processes all image files in input_folder:\n",
    "      - Runs predictions using get_prediction function and detection_model.\n",
    "      - Saves annotated images with bounding boxes in output_folder.\n",
    "      - Saves prediction details as JSON files in output_folder.\n",
    "    \n",
    "    Parameters:\n",
    "      input_folder (str): Path to the folder containing images.\n",
    "      output_folder (str): Path to the folder where results will be saved.\n",
    "      detection_model: Your detection model used for prediction.\n",
    "    \"\"\"\n",
    "    name = \"exp\"\n",
    "    save_dir = Path(increment_path(Path(\"sliced_predictions\") / name, exist_ok=False))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if dataset_json_path:\n",
    "        with open(dataset_json_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "    \n",
    "    #color = (0, 255, 0)  # original annotations in green\n",
    "    visual_bbox_gt_thickness = 3\n",
    "    visual_bbox_thickness = 2\n",
    "    visual_text_size = 0.5\n",
    "    visual_text_thickness = 1\n",
    "    visual_hide_labels = False\n",
    "    visual_hide_conf = False\n",
    "    visual_export_format = 'png'\n",
    "    sliced_predictions = []\n",
    "    image_ids = []\n",
    "    coco_json = []\n",
    "    \n",
    "    # Loop over files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            image_as_pil = read_image_as_pil(image_path)\n",
    "            filename_without_ext = Path(filename).stem\n",
    "            \n",
    "            print(\"*****************************************\")\n",
    "            print(\"File Name\", filename_without_ext)\n",
    "\n",
    "            img_id = get_image_id(data, filename_without_ext)\n",
    "            #image_ids.append(image_id)\n",
    "            \n",
    "            # Get predictions from your detection model\n",
    "            time_start = time.time()\n",
    "            prediction = get_prediction(image_path, detection_model)\n",
    "            time_end = time.time() - time_start\n",
    "            #print(f\"Prediction Performed in {time_end} seconds\")\n",
    "            print(\"Initial Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "            \n",
    "            object_density = len(prediction.object_prediction_list)\n",
    "            print(\"Object Density:\", object_density)\n",
    "            \n",
    "            slice_width, slice_height, overlap_width_ratio, overlap_height_ratio = get_slice_parameters(object_density)\n",
    "\n",
    "            print(\"********* Slice Parameters ***********\")\n",
    "            print(\"Slice Width: \", slice_width)\n",
    "            print(\"Slice Height: \", slice_height)\n",
    "            print(\"Overlap Width Ratio: \", overlap_width_ratio)\n",
    "            print(\"Overlap Height Ratio: \", overlap_height_ratio)\n",
    "\n",
    "            time_start_slice = time.time()\n",
    "            result_sahi = get_sliced_prediction(\n",
    "                image_path,\n",
    "                slice_detection_model,\n",
    "                slice_height = slice_height,\n",
    "                slice_width = slice_width,\n",
    "                overlap_height_ratio = overlap_height_ratio,\n",
    "                overlap_width_ratio = overlap_width_ratio,\n",
    "                postprocess_type = \"OptNMS\",\n",
    "                verbose = 2\n",
    "            )\n",
    "            time_end_slice = time.time() - time_start_slice\n",
    "            #print(f\"Prediction Performed in {time_end1} seconds\")\n",
    "            print(\"Prediction time is: {:.2f} ms\".format(time_end_slice * 1000))\n",
    "            \n",
    "            coco_prediction = result_sahi.to_coco_predictions(image_id=img_id)\n",
    "\n",
    "            for idx, predict in enumerate(coco_prediction):\n",
    "                if coco_prediction[idx][\"bbox\"]:\n",
    "                        coco_json.append(predict)\n",
    "                \n",
    "            sliced_predictions.append(result_sahi)\n",
    "            \n",
    "            visualize_object_predictions(\n",
    "                np.ascontiguousarray(image_as_pil),\n",
    "                object_prediction_list=result_sahi.object_prediction_list,\n",
    "                rect_th=visual_bbox_thickness,\n",
    "                text_size=visual_text_size,\n",
    "                text_th=visual_text_thickness,\n",
    "                hide_labels=visual_hide_labels,\n",
    "                hide_conf=visual_hide_conf,\n",
    "                output_dir=save_dir,\n",
    "                file_name=filename_without_ext,\n",
    "                export_format=visual_export_format,\n",
    "            )\n",
    "            \n",
    "        total_time = time_end + time_end_slice\n",
    "     \n",
    "    if dataset_json_path:\n",
    "        save_path = str(save_dir / \"result.json\")\n",
    "        save_json(coco_json, save_path)\n",
    "        print(f\"Prediction results are successfully exported to {save_dir}\")\n",
    "    print(f\"Prediction Completed Sucessfully: {len(sliced_predictions)} images\")\n",
    "    print(\"Total Prediction time is: {:.2f} ms\".format(total_time * 1000))\n",
    "    return sliced_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f035d1e-a8cc-43c8-b349-46fe10746ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sahi.prediction import ObjectPrediction, PredictionResult\n",
    "\n",
    "# export visualization\n",
    "def predict_sliced_images(input_folder, dataset_json_path, detection_model, slice_detection_model):\n",
    "    \"\"\"\n",
    "    Processes all image files in input_folder:\n",
    "      - Runs predictions using get_prediction function and detection_model.\n",
    "      - Saves annotated images with bounding boxes in output_folder.\n",
    "      - Saves prediction details as JSON files in output_folder.\n",
    "    \n",
    "    Parameters:\n",
    "      input_folder (str): Path to the folder containing images.\n",
    "      output_folder (str): Path to the folder where results will be saved.\n",
    "      detection_model: Your detection model used for prediction.\n",
    "    \"\"\"\n",
    "    name = \"exp\"\n",
    "    save_dir = Path(increment_path(Path(\"sliced_predictions\") / name, exist_ok=False))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if dataset_json_path:\n",
    "        with open(dataset_json_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "    \n",
    "    #color = (0, 255, 0)  # original annotations in green\n",
    "    visual_bbox_gt_thickness = 3\n",
    "    visual_bbox_thickness = 2\n",
    "    visual_text_size = 0.5\n",
    "    visual_text_thickness = 1\n",
    "    visual_hide_labels = False\n",
    "    visual_hide_conf = False\n",
    "    visual_export_format = 'png'\n",
    "    sliced_predictions = []\n",
    "    image_ids = []\n",
    "    coco_json = []\n",
    "    \n",
    "    # Loop over files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            image_as_pil = read_image_as_pil(image_path)\n",
    "            filename_without_ext = Path(filename).stem\n",
    "            image_width, image_height = image_as_pil.size\n",
    "            print(\"*****************************************\")\n",
    "            print(\"File Name\", filename_without_ext)\n",
    "            #print(\"Img Ht: \", image_height)\n",
    "            img_id = get_image_id(data, filename_without_ext)\n",
    "            #image_ids.append(image_id)\n",
    "            \n",
    "            # Get predictions from your detection model\n",
    "            time_start = time.time()\n",
    "            prediction = get_prediction(image_path, detection_model)\n",
    "            time_end = time.time() - time_start\n",
    "            #print(f\"Prediction Performed in {time_end} seconds\")\n",
    "            print(\"Initial Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "            \n",
    "            object_density = len(prediction.object_prediction_list)\n",
    "            print(\"Object Density:\", object_density)\n",
    "            \n",
    "            if object_density > 5:\n",
    "                slice_width, slice_height, overlap_width_ratio, overlap_height_ratio = get_slice_parameters(object_density)\n",
    "    \n",
    "                print(\"********* Slice Parameters ***********\")\n",
    "                print(\"Slice Width: \", slice_width)\n",
    "                print(\"Slice Height: \", slice_height)\n",
    "                print(\"Overlap Width Ratio: \", overlap_width_ratio)\n",
    "                print(\"Overlap Height Ratio: \", overlap_height_ratio)\n",
    "\n",
    "                time_start_slice = time.time()\n",
    "                result_sahi = get_sliced_prediction(\n",
    "                    image_path,\n",
    "                    slice_detection_model,\n",
    "                    slice_height = slice_height,\n",
    "                    slice_width = slice_width,\n",
    "                    overlap_height_ratio = overlap_height_ratio,\n",
    "                    overlap_width_ratio = overlap_width_ratio,\n",
    "                    postprocess_type = \"OptNMS\",\n",
    "                    verbose = 2\n",
    "                )\n",
    "                time_end_slice = time.time() - time_start_slice\n",
    "                #print(f\"Prediction Performed in {time_end1} seconds\")\n",
    "                print(\"Prediction time is: {:.2f} ms\".format(time_end_slice * 1000))\n",
    "                \n",
    "                coco_prediction = result_sahi.to_coco_predictions(image_id=img_id)\n",
    "\n",
    "                for idx, predict in enumerate(coco_prediction):\n",
    "                    if coco_prediction[idx][\"bbox\"]:\n",
    "                            coco_json.append(predict)\n",
    "                    \n",
    "                sliced_predictions.append(result_sahi)\n",
    "                \n",
    "                visualize_object_predictions(\n",
    "                    np.ascontiguousarray(image_as_pil),\n",
    "                    object_prediction_list=result_sahi.object_prediction_list,\n",
    "                    rect_th=visual_bbox_thickness,\n",
    "                    text_size=visual_text_size,\n",
    "                    text_th=visual_text_thickness,\n",
    "                    hide_labels=visual_hide_labels,\n",
    "                    hide_conf=visual_hide_conf,\n",
    "                    output_dir=save_dir,\n",
    "                    file_name=filename_without_ext,\n",
    "                    export_format=visual_export_format,\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                print(\"Prediction time is: {:.2f} ms\".format(time_end * 1000))\n",
    "                \n",
    "                coco_prediction = prediction.to_coco_predictions(image_id=img_id)\n",
    "\n",
    "                for idx, predict in enumerate(coco_prediction):\n",
    "                    if coco_prediction[idx][\"bbox\"]:\n",
    "                            coco_json.append(predict)\n",
    "                    \n",
    "                sliced_predictions.append(prediction)\n",
    "                \n",
    "                visualize_object_predictions(\n",
    "                    np.ascontiguousarray(image_as_pil),\n",
    "                    object_prediction_list=prediction.object_prediction_list,\n",
    "                    rect_th=visual_bbox_thickness,\n",
    "                    text_size=visual_text_size,\n",
    "                    text_th=visual_text_thickness,\n",
    "                    hide_labels=visual_hide_labels,\n",
    "                    hide_conf=visual_hide_conf,\n",
    "                    output_dir=save_dir,\n",
    "                    file_name=filename_without_ext,\n",
    "                    export_format=visual_export_format,\n",
    "                )\n",
    "        total_time = time_end + time_end_slice\n",
    "     \n",
    "    if dataset_json_path:\n",
    "        save_path = str(save_dir / \"result.json\")\n",
    "        save_json(coco_json, save_path)\n",
    "        print(f\"Prediction results are successfully exported to {save_dir}\")\n",
    "    print(f\"Prediction Completed Sucessfully: {len(sliced_predictions)} images\")\n",
    "    print(\"Total Prediction time is: {:.2f} ms\".format(total_time * 1000))\n",
    "    return sliced_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d80d9f98-314f-428f-a86e-77a5af72e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST PROCESSING: NMS\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|██████████| 1/1 [00:00<00:00, 210.24it/s]\n",
      "Performing inference on images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: 0000074_08202_d_0000016\n",
      "Image Size:  (1920, 1080)\n",
      "Sliced Boxes Count: 112\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 112 slices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   0%|          | 0/1 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction Count 561\n",
      "Final Bounding Box Count (NMS):  89\n",
      "Final Bounding Box Count (NMS):  2\n",
      "Final Bounding Box Count (NMS):  64\n",
      "Final Bounding Box Count (NMS):  7\n",
      "Final Bounding Box Count (NMS):  6\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  14\n",
      "Final Bounding Box Count (NMS):  6\n",
      "Prediction time is: 1023.01 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results are successfully exported to runs/predict/exp313\n",
      "Model loaded in 0.0296480655670166 seconds.\n",
      "Slicing performed in 0.002722501754760742 seconds.\n",
      "Prediction performed in 1.0230062007904053 seconds.\n",
      "Exporting performed in 0.08468127250671387 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_predict_nms = predict(source='./single_test/test',\n",
    "                         dataset_json_path = './subset_vis_test_data_889.json',\n",
    "                         model_type = 'ultralytics',\n",
    "                         model_path = 'models/yolov8/last.pt',\n",
    "                         slice_height = 256,\n",
    "                         slice_width = 256,\n",
    "                         overlap_height_ratio = 0.5,\n",
    "                         overlap_width_ratio = 0.5,\n",
    "                         postprocess_type = \"NMS\",\n",
    "                         postprocess_min_area = 16,\n",
    "                         postprocess_conf_threshold = 0.3,                  \n",
    "                         verbose = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41a85949-3968-46d7-bc8d-d5013054f748",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'time_end' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m source_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./single_test/test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./subset_vis_test_data_889.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m result_preds_adaptive_single_model_11 \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_sliced_images_modified\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 109\u001b[0m, in \u001b[0;36mpredict_sliced_images_modified\u001b[0;34m(input_folder, dataset_json_path, detection_model, slice_detection_model)\u001b[0m\n\u001b[1;32m     94\u001b[0m         sliced_predictions\u001b[38;5;241m.\u001b[39mappend(result_sahi)\n\u001b[1;32m     96\u001b[0m         visualize_object_predictions(\n\u001b[1;32m     97\u001b[0m             np\u001b[38;5;241m.\u001b[39mascontiguousarray(image_as_pil),\n\u001b[1;32m     98\u001b[0m             object_prediction_list\u001b[38;5;241m=\u001b[39mresult_sahi\u001b[38;5;241m.\u001b[39mobject_prediction_list,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m             export_format\u001b[38;5;241m=\u001b[39mvisual_export_format,\n\u001b[1;32m    107\u001b[0m         )\n\u001b[0;32m--> 109\u001b[0m     total_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime_end\u001b[49m \u001b[38;5;241m+\u001b[39m time_end_slice\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_json_path:\n\u001b[1;32m    112\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'time_end' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/test'\n",
    "json_path = \"./subset_vis_test_data_889.json\"\n",
    "result_preds_adaptive_single_model_11 = predict_sliced_images_modified(source_folder, json_path, detection_model, detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a712fb9-79c9-421a-a5d3-1b3723c4378c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST PROCESSING: NMS\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|██████████| 1/1 [00:00<00:00, 203.75it/s]\n",
      "Performing inference on images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: 0000074_08202_d_0000016\n",
      "Image Size:  (1920, 1080)\n",
      "Sliced Boxes Count: 28\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 28 slices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images:   0%|          | 0/1 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction Count 869\n",
      "Final Bounding Box Count (NMS):  124\n",
      "Final Bounding Box Count (NMS):  2\n",
      "Final Bounding Box Count (NMS):  7\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  1\n",
      "Final Bounding Box Count (NMS):  2\n",
      "Prediction time is: 3086.98 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images: 100%|██████████| 1/1 [00:03<00:00,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results are successfully exported to runs/predict/exp312\n",
      "Model loaded in 0.03259158134460449 seconds.\n",
      "Slicing performed in 0.0022890567779541016 seconds.\n",
      "Prediction performed in 3.0869834423065186 seconds.\n",
      "Exporting performed in 0.09447598457336426 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_predict_nms = predict(source='./single_test/test',\n",
    "                         dataset_json_path = './subset_vis_test_data_889.json',\n",
    "                         model_type = 'ultralytics',\n",
    "                         model_path = 'models/yolov8/last.pt',\n",
    "                         slice_height = 512,\n",
    "                         slice_width = 512,\n",
    "                         overlap_height_ratio = 0.5,\n",
    "                         overlap_width_ratio = 0.5,\n",
    "                         postprocess_type = \"NMS\",\n",
    "                         postprocess_min_area = 16,\n",
    "                         postprocess_conf_threshold = 0.3,                  \n",
    "                         verbose = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0935a82b-5693-45dd-8799-311cd2bfb891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000074_08202_d_0000016\n",
      "Initial Prediction time is: 36.44 ms\n",
      "Object Density: 50\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.5\n",
      "Overlap Height Ratio:  0.5\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 28 slices.\n",
      "Original Prediction Count 672\n",
      "Filtered Prediction:  658\n",
      "Final Bounding Box Count: 126\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  10\n",
      "Final Bounding Box Count: 6\n",
      "Filtered Prediction:  3\n",
      "Final Bounding Box Count: 1\n",
      "Slicing performed in 0.010401248931884766 seconds.\n",
      "Prediction performed in 0.5085651874542236 seconds.\n",
      "Prediction time is: 516.98 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp123\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 553.42 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/test'\n",
    "json_path = \"./subset_vis_test_data_889.json\"\n",
    "result_preds_adaptive_single_model_11 = predict_sliced_images(source_folder, json_path, detection_model, detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "98c65285-9d09-4bc5-b47e-92e9219b911f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 9999938_00000_d_0000207\n",
      "Initial Prediction time is: 91.17 ms\n",
      "Object Density: 24\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 8 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [888, 0]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 103\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  41\n",
      "Final Bounding Box Count: 40\n",
      "Filtered Prediction:  43\n",
      "Final Bounding Box Count: 41\n",
      "Filtered Prediction:  7\n",
      "Final Bounding Box Count: 6\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  9\n",
      "Final Bounding Box Count: 9\n",
      "Filtered Prediction:  2\n",
      "Final Bounding Box Count: 2\n",
      "Slicing performed in 0.008027076721191406 seconds.\n",
      "Prediction performed in 0.1379075050354004 seconds.\n",
      "Prediction time is: 144.71 ms\n",
      "*****************************************\n",
      "File Name 0000006_05208_d_0000014\n",
      "Initial Prediction time is: 23.82 ms\n",
      "Object Density: 17\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 6 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [0, 253]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 34\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  2\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  31\n",
      "Final Bounding Box Count: 6\n",
      "Slicing performed in 0.005963563919067383 seconds.\n",
      "Prediction performed in 0.06387734413146973 seconds.\n",
      "Prediction time is: 68.63 ms\n",
      "*****************************************\n",
      "File Name 0000370_02000_d_0000254\n",
      "Initial Prediction time is: 23.85 ms\n",
      "Object Density: 2\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  1024\n",
      "Slice Height:  1024\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 2 slices.\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 2\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  2\n",
      "Final Bounding Box Count: 2\n",
      "Slicing performed in 0.00543522834777832 seconds.\n",
      "Prediction performed in 0.028081893920898438 seconds.\n",
      "Prediction time is: 32.33 ms\n",
      "*****************************************\n",
      "File Name 0000006_06773_d_0000018\n",
      "Initial Prediction time is: 24.49 ms\n",
      "Object Density: 37\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.25\n",
      "Overlap Height Ratio:  0.25\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 8 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [848, 0]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 45\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  26\n",
      "Final Bounding Box Count: 17\n",
      "Filtered Prediction:  2\n",
      "Final Bounding Box Count: 2\n",
      "Filtered Prediction:  13\n",
      "Final Bounding Box Count: 8\n",
      "Filtered Prediction:  3\n",
      "Final Bounding Box Count: 2\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Slicing performed in 0.006483793258666992 seconds.\n",
      "Prediction performed in 0.06759071350097656 seconds.\n",
      "Prediction time is: 73.00 ms\n",
      "*****************************************\n",
      "File Name 0000006_05999_d_0000017\n",
      "Initial Prediction time is: 25.22 ms\n",
      "Object Density: 59\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.5\n",
      "Overlap Height Ratio:  0.5\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 10 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [768, 0]\n",
      "SHIFT AMOUNT:  [256, 253]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 92\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  18\n",
      "Final Bounding Box Count: 16\n",
      "Filtered Prediction:  8\n",
      "Final Bounding Box Count: 7\n",
      "Filtered Prediction:  46\n",
      "Final Bounding Box Count: 26\n",
      "Filtered Prediction:  20\n",
      "Final Bounding Box Count: 13\n",
      "Slicing performed in 0.006203174591064453 seconds.\n",
      "Prediction performed in 0.08693528175354004 seconds.\n",
      "Prediction time is: 92.04 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp94\n",
      "Prediction Completed Sucessfully: 5 images\n",
      "Total Prediction time is: 117.26 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Modified Adaptive-NMS\n",
    "source_folder = './test_data/images'\n",
    "json_path = \"./subset_visdrone_test_data.json\"\n",
    "result_preds_adaptive_single_model_different_conf3 = predict_sliced_images_modified(source_folder, json_path, detection_model, slice_detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6fae916a-e711-479d-bb55-9de4559407b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.19s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.168\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.253\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.192\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.281\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.463\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.246\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.198\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.318\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.120\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.207\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.428\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.159\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp94/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive modified single METHOD (201 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_visdrone_test_data.json' --result_json_path './sliced_predictions/exp94/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01fc2df5-8673-44c9-ab5a-8b7a495aa967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000078_03171_d_0000009\n",
      "Initial Prediction time is: 23.37 ms\n",
      "Object Density: 55\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.5\n",
      "Overlap Height Ratio:  0.5\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 10 slices.\n",
      "Original Prediction Count 246\n",
      "Filtered Prediction:  33\n",
      "Final Bounding Box Count: 19\n",
      "Filtered Prediction:  8\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  11\n",
      "Final Bounding Box Count: 6\n",
      "Filtered Prediction:  190\n",
      "Final Bounding Box Count: 50\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  3\n",
      "Final Bounding Box Count: 2\n",
      "Slicing performed in 0.005565643310546875 seconds.\n",
      "Prediction performed in 0.19089388847351074 seconds.\n",
      "Prediction time is: 195.49 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp124\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 218.86 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_10.json\"\n",
    "result_preds_adaptive_single_model_different_conf4 = predict_sliced_images(source_folder, json_path, detection_model, detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a1ce7af7-4ac4-48c5-8a31-5beee08c1714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000078_03171_d_0000009\n",
      "Initial Prediction time is: 24.96 ms\n",
      "Object Density: 8\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  1024\n",
      "Slice Height:  1024\n",
      "Overlap Width Ratio:  0.15\n",
      "Overlap Height Ratio:  0.15\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 2 slices.\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 55\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  6\n",
      "Final Bounding Box Count: 6\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  47\n",
      "Final Bounding Box Count: 32\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Slicing performed in 0.00618290901184082 seconds.\n",
      "Prediction performed in 0.03507518768310547 seconds.\n",
      "Prediction time is: 39.95 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp93\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 49.91 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_10.json\"\n",
    "result_preds_adaptive_single_model_different_conf3 = predict_sliced_images(source_folder, json_path, detection_model, slice_detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "79a7f826-e4e7-481e-8f05-8cbe4beaa831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dsu.local/bshakya/.local/bin/sahi\", line 8, in <module>\n",
      "    sys.exit(app())\n",
      "  File \"/home/dsu.local/bshakya/.local/lib/python3.9/site-packages/sahi/cli.py\", line 32, in app\n",
      "    fire.Fire(sahi_app)\n",
      "  File \"/home/dsu.local/bshakya/.local/lib/python3.9/site-packages/fire/core.py\", line 135, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"/home/dsu.local/bshakya/.local/lib/python3.9/site-packages/fire/core.py\", line 468, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "  File \"/home/dsu.local/bshakya/.local/lib/python3.9/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/home/dsu.local/bshakya/.local/lib/python3.9/site-packages/sahi/scripts/coco_evaluation.py\", line 362, in evaluate\n",
      "    result = evaluate_core(\n",
      "  File \"/home/dsu.local/bshakya/.local/lib/python3.9/site-packages/sahi/scripts/coco_evaluation.py\", line 128, in evaluate_core\n",
      "    cocoDt = cocoGt.loadRes(results)\n",
      "  File \"/home/dsu.local/bshakya/.local/lib/python3.9/site-packages/pycocotools/coco.py\", line 327, in loadRes\n",
      "    assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n",
      "AssertionError: Results do not correspond to current coco set\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive conf single METHOD (44 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_10.json' --result_json_path './sliced_predictions/exp93/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6a67e7ab-483d-4604-b96d-a4c2f3f4ec1d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.140\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.230\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.142\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.121\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.415\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.067\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.154\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.706\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.071\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.168\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.750\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp89/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive conf single METHOD (44 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp89/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "609adf5c-d12e-4ee2-aa1e-ee30a11e7d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000078_03171_d_0000009\n",
      "Initial Prediction time is: 87.43 ms\n",
      "Object Density: 30\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.25\n",
      "Overlap Height Ratio:  0.25\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 8 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [848, 0]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 63\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  10\n",
      "Final Bounding Box Count: 10\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  51\n",
      "Final Bounding Box Count: 28\n",
      "Slicing performed in 0.005843639373779297 seconds.\n",
      "Prediction performed in 0.12724924087524414 seconds.\n",
      "Prediction time is: 131.87 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp85\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 219.30 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1563.json\"\n",
    "result_preds_adaptive_single_model_different_conf2 = predict_sliced_images(source_folder, json_path, detection_model, slice_detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6c3e0aff-fc39-4fbc-9811-cfc679c896e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.090\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.178\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.080\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.157\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.420\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.417\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.081\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.159\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.283\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.085\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.172\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.300\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp85/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive conf single METHOD (44 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp85/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "72936df8-e160-4c7a-a1a1-51e1488bdc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000078_03171_d_0000009\n",
      "Initial Prediction time is: 85.06 ms\n",
      "Object Density: 30\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.25\n",
      "Overlap Height Ratio:  0.25\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 8 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [848, 0]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 101\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  16\n",
      "Final Bounding Box Count: 15\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  82\n",
      "Final Bounding Box Count: 40\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Slicing performed in 0.0058133602142333984 seconds.\n",
      "Prediction performed in 0.13054871559143066 seconds.\n",
      "Prediction time is: 136.18 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp84\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 221.24 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1563.json\"\n",
    "result_preds_adaptive_single_model_different_conf1 = predict_sliced_images(source_folder, json_path, detection_model, slice_detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d61499e7-d936-456d-b142-245c44153760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.147\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.259\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.143\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.178\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.452\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.751\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.088\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.169\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.550\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.094\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.184\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.567\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp84/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive conf single METHOD (44 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp84/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e98ba3f-7aaa-4c59-a909-d453f68a5687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000078_03171_d_0000009\n",
      "Initial Prediction time is: 23.04 ms\n",
      "Object Density: 55\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.5\n",
      "Overlap Height Ratio:  0.5\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 10 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [768, 0]\n",
      "SHIFT AMOUNT:  [256, 253]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 70\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  10\n",
      "Final Bounding Box Count: 10\n",
      "Filtered Prediction:  3\n",
      "Final Bounding Box Count: 3\n",
      "Filtered Prediction:  57\n",
      "Final Bounding Box Count: 31\n",
      "Slicing performed in 0.005728960037231445 seconds.\n",
      "Prediction performed in 0.08202123641967773 seconds.\n",
      "Prediction time is: 86.65 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp83\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 109.69 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1563.json\"\n",
    "result_preds_adaptive_single_model_different_conf = predict_sliced_images(source_folder, json_path, detection_model, slice_detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f492ebd0-4131-4999-8c58-3e00597a72b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.088\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.165\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.078\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.174\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.411\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.417\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.091\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.156\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.275\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.098\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.283\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp83/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive conf single METHOD (44 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp83/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5145be4d-814e-4eb8-926e-8e0fc1294046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000078_03171_d_0000009\n",
      "Object Density: 55\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.5\n",
      "Overlap Height Ratio:  0.5\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 10 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [768, 0]\n",
      "SHIFT AMOUNT:  [256, 253]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 116\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  22\n",
      "Final Bounding Box Count: 19\n",
      "Filtered Prediction:  2\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  3\n",
      "Final Bounding Box Count: 3\n",
      "Filtered Prediction:  88\n",
      "Final Bounding Box Count: 40\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Slicing performed in 0.006308555603027344 seconds.\n",
      "Prediction performed in 0.08841061592102051 seconds.\n",
      "Prediction time is: 93.34 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp79\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 121.95 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1563.json\"\n",
    "result_preds_adaptive_single_model = predict_sliced_images(source_folder, json_path, slice_detection_model, slice_detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8b04d47-eed8-4db5-9a88-4040728dc8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000078_03171_d_0000009\n",
      "Object Density: 44\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.25\n",
      "Overlap Height Ratio:  0.25\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 8 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [848, 0]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 83\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  14\n",
      "Final Bounding Box Count: 13\n",
      "Filtered Prediction:  4\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  63\n",
      "Final Bounding Box Count: 34\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Slicing performed in 0.006582975387573242 seconds.\n",
      "Prediction performed in 0.07649707794189453 seconds.\n",
      "Prediction time is: 81.38 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp80\n",
      "Prediction Completed Sucessfully: 1 images\n",
      "Total Prediction time is: 112.67 ms\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1563.json\"\n",
    "result_preds_adaptive_single_model_11 = predict_sliced_images(source_folder, json_path, detection_model, detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b0b6b97-8c9e-469e-9457-6ae0f4dc86a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.134\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.237\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.115\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.147\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.415\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.080\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.194\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.633\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.088\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.216\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.633\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp80/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive single METHOD (50 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp80/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39a6ee2f-0efd-498a-ac3f-ca336b666641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000078_03171_d_0000009\n",
      "Object Density: 55\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.5\n",
      "Overlap Height Ratio:  0.5\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 10 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [768, 0]\n",
      "SHIFT AMOUNT:  [256, 253]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 116\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  22\n",
      "Final Bounding Box Count: 19\n",
      "Filtered Prediction:  2\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  3\n",
      "Final Bounding Box Count: 3\n",
      "Filtered Prediction:  88\n",
      "Final Bounding Box Count: 40\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Slicing performed in 0.005620002746582031 seconds.\n",
      "Prediction performed in 0.08562970161437988 seconds.\n",
      "Prediction time is: 90.17 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp76\n",
      "Prediction Completed Sucessfully: 1 images\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1563.json\"\n",
    "result_preds_adaptive_single_model = predict_sliced_images(source_folder, json_path, detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ffa6f328-b366-43ad-9697-de70dfa34a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.151\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.265\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.148\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.215\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.426\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.751\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.104\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.161\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.542\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.114\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.174\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.550\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp79/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive single METHOD (64 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp79/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "911de174-9b10-49dd-aadb-ca78ead49f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.151\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.265\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.148\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.215\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.426\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.751\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.104\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.161\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.542\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.114\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.174\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.550\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp76/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive single METHOD (64 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp76/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50a2973e-018b-40ed-8791-aa674eced44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000078_03171_d_0000009\n",
      "Object Density: 55\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.5\n",
      "Overlap Height Ratio:  0.5\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 10 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [768, 0]\n",
      "SHIFT AMOUNT:  [256, 253]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 111\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  24\n",
      "Final Bounding Box Count: 20\n",
      "Filtered Prediction:  3\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  81\n",
      "Final Bounding Box Count: 41\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Slicing performed in 0.00595402717590332 seconds.\n",
      "Prediction performed in 0.16543340682983398 seconds.\n",
      "Prediction time is: 171.73 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp77\n",
      "Prediction Completed Sucessfully: 1 images\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS (65 objects)\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1563.json\"\n",
    "result_preds_adaptive_single_multi_model = predict_sliced_images(source_folder, json_path, detection_model, slice_detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5f57e0b-dbdb-42de-bdc3-9f70e342d79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.144\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.253\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.129\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.199\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.419\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.889\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.104\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.206\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.556\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.114\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.224\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.600\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp77/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive single METHOD (65 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp77/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53850028-6368-4ef3-a71e-6e77b4bf07cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "File Name 0000078_03171_d_0000009\n",
      "Object Density: 44\n",
      "********* Slice Parameters ***********\n",
      "Slice Width:  512\n",
      "Slice Height:  512\n",
      "Overlap Width Ratio:  0.25\n",
      "Overlap Height Ratio:  0.25\n",
      "POST PROCESS:  OptNMS\n",
      "Performing prediction on 8 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [848, 0]\n",
      "CLASS TYPE:  False\n",
      "Original Prediction Count 101\n",
      "****non_class_agnostic****\n",
      "Filtered Prediction:  16\n",
      "Final Bounding Box Count: 15\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Filtered Prediction:  82\n",
      "Final Bounding Box Count: 40\n",
      "Filtered Prediction:  1\n",
      "Final Bounding Box Count: 1\n",
      "Slicing performed in 0.005623340606689453 seconds.\n",
      "Prediction performed in 0.12740802764892578 seconds.\n",
      "Prediction time is: 131.90 ms\n",
      "Prediction results are successfully exported to sliced_predictions/exp78\n",
      "Prediction Completed Sucessfully: 1 images\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Adaptive-NMS\n",
    "source_folder = './single_test/images'\n",
    "json_path = \"./subset_vis_test_data_1563.json\"\n",
    "result_preds_adaptive_single_multi_first_11_model = predict_sliced_images(source_folder, json_path, detection_model, slice_detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e7d1b4e-d2d8-48b1-bd7d-ef8fada1ca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.147\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.259\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.143\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.178\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.452\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.751\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.088\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.169\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.550\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.094\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.184\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.567\n",
      "COCO evaluation results are successfully exported to sliced_predictions/exp78/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING Adaptive single METHOD (58 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './sliced_predictions/exp78/result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "028b51b8-8ec3-43a4-8113-0b79766330e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST PROCESSING: NMS\n",
      "indexing coco dataset annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading coco annotations: 100%|██████████| 1/1 [00:00<00:00, 195.64it/s]\n",
      "Performing inference on images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: 0000078_03171_d_0000009\n",
      "Image Size:  (1360, 765)\n",
      "POST PROCESS:  NMS\n",
      "Performing prediction on 10 slices.\n",
      "SHIFT AMOUNT:  [0, 0]\n",
      "SHIFT AMOUNT:  [768, 0]\n",
      "SHIFT AMOUNT:  [256, 253]\n",
      "Total Valid prediction:  19\n",
      "Total Valid prediction:  1\n",
      "Total Valid prediction:  3\n",
      "Total Valid prediction:  42\n",
      "Total Valid prediction:  1\n",
      "Total Valid prediction:  1\n",
      "Prediction time is: 132.20 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing inference on images: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results are successfully exported to runs/predict/exp168\n",
      "Model loaded in 0.03319692611694336 seconds.\n",
      "Slicing performed in 0.0012388229370117188 seconds.\n",
      "Prediction performed in 0.13219690322875977 seconds.\n",
      "Exporting performed in 0.04612278938293457 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_predict_nms = predict(source='./single_test',\n",
    "                         dataset_json_path = './subset_vis_test_data_1563.json',\n",
    "                         model_type = 'ultralytics',\n",
    "                         model_path = 'models/yolov8/last.pt',\n",
    "                         slice_height = 512,\n",
    "                         slice_width = 512,\n",
    "                         overlap_height_ratio = 0.5,\n",
    "                         overlap_width_ratio = 0.5,\n",
    "                         postprocess_type = \"NMS\",\n",
    "                         verbose = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d39fd80-ca17-47f5-b0c1-a99bc72af05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=500 ] = 0.153\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=500 ] = 0.268\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=500 ] = 0.151\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= small | maxDets=500 ] = 0.228\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=medium | maxDets=500 ] = 0.426\n",
      " Average Precision  (AP) @[ IoU=0.50      | area= large | maxDets=500 ] = 0.751\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.108\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.161\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.585\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=500 ] = 0.120\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=500 ] = 0.174\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=500 ] = 0.667\n",
      "COCO evaluation results are successfully exported to runs/predict/exp168/eval.json\n"
     ]
    }
   ],
   "source": [
    "#USING NMS METHOD (67 objects)\n",
    "!sahi coco evaluate --dataset_json_path './subset_vis_test_data_1563.json' --result_json_path './runs/predict/exp168/result.json'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch11.8]",
   "language": "python",
   "name": "conda-env-torch11.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
